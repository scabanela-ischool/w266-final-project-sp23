{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74baf79b",
   "metadata": {},
   "source": [
    "Follow these tutorials:\n",
    "\n",
    "- For BERTweet: https://www.kaggle.com/code/tylerrosacker/bertweet-transfer-learning\n",
    "- Class notebook: https://github.com/datasci-w266/2023-spring-main/blob/master/materials/walkthrough_notebooks/bert_as_black_box/Keras_HuggingFace_Transformers_BERT_notebook.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e04c2c2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f709325",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a111b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji==0.6.0\n",
      "  Downloading emoji-0.6.0.tar.gz (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m325.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for emoji: filename=emoji-0.6.0-py3-none-any.whl size=49720 sha256=2efb9c2c6724a99be3b6cf5cb19440b017df2a7c12fea85089439b7df62abf11\n",
      "  Stored in directory: /Users/cabanela/Library/Caches/pip/wheels/1b/bd/d9/310c33c45a553798a714e27e3b8395d37128425442b8c78e07\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-0.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install emoji==0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c687cabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "# import tensorflow_addons as tfa\n",
    "import transformers\n",
    "from transformers import AutoTokenizer,TFRobertaModel\n",
    "# from transformers import AutoTokenizer,AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "657321c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.27.4\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48fb176",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9500242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from csv files\n",
    "disability_df_train = pd.read_csv('data/disability-dataset-train.csv')\n",
    "disability_df_val = pd.read_csv('data/disability-dataset-val.csv')\n",
    "disability_df_test = pd.read_csv('data/disability-dataset-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd06be05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "disability_df_train = disability_df_train.copy().sample(frac=1, random_state=266)\n",
    "disability_df_val = disability_df_val.copy().sample(frac=1, random_state=266)\n",
    "disability_df_test = disability_df_test.copy().sample(frac=1, random_state=266)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881a5e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Form tensors of labels and features.\n",
    "# disability_train_labels = tf.convert_to_tensor(disability_df_train['toxicity_binary'])\n",
    "# disability_val_labels = tf.convert_to_tensor(disability_df_val['toxicity_binary'])\n",
    "# disability_test_labels = tf.convert_to_tensor(disability_df_test['toxicity_binary'])\n",
    "\n",
    "# disability_train_examples = tf.convert_to_tensor(disability_df_train['comment_text'])\n",
    "# disability_val_examples = tf.convert_to_tensor(disability_df_val['comment_text'])\n",
    "# disability_test_examples = tf.convert_to_tensor(disability_df_test['comment_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf372ab7",
   "metadata": {},
   "source": [
    "## Tokenize Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e487f7dc",
   "metadata": {},
   "source": [
    "Download the tokenizer corresponding to BERTweet from HuggingFace:\n",
    "**TODO: Evaluate whether we need to download emoji tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "794cbec7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# For transformers v4.x+:\n",
    "bertweet_tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e97979",
   "metadata": {},
   "source": [
    "EDA revealed that most comments have 128 words or less, so we'll set MAX_SEQUENCE_LENGTH to 128 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "904a465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_SEQUENCE_LENGTH = 128\n",
    "# batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "54db8ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_function(examples):\n",
    "#     return bertweet_tokenizer(examples[\"comment_text\"].tolist(), max_length = MAX_SEQUENCE_LENGTH, padding=\"max_length\", truncation=True, return_tensors = 'tf')\n",
    "\n",
    "# tf_train_dataset = disability_df_train.map(tokenize_function, batched=True)\n",
    "# tf_val_dataset = disability_df_val.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06ac8727",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 128\n",
    "batch_size = 32\n",
    "\n",
    "tf_train_dataset = bertweet_tokenizer(disability_df_train['comment_text'].tolist(), \n",
    "                    max_length = MAX_SEQUENCE_LENGTH,\n",
    "                    padding=\"max_length\", \n",
    "                    truncation=True,\n",
    "                    return_tensors = 'tf').data\n",
    "tf_val_dataset = bertweet_tokenizer(disability_df_val['comment_text'].tolist(), \n",
    "                    max_length = MAX_SEQUENCE_LENGTH,\n",
    "                    padding=\"max_length\", \n",
    "                    truncation=True,\n",
    "                    return_tensors = 'tf').data\n",
    "tf_test_dataset = bertweet_tokenizer(disability_df_test['comment_text'].tolist(), \n",
    "                    max_length = MAX_SEQUENCE_LENGTH,\n",
    "                    padding=\"max_length\", \n",
    "                    truncation=True,\n",
    "                    return_tensors = 'tf').data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b10137ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([13438, 128])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_train_dataset['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e328d278",
   "metadata": {},
   "source": [
    "## Encode Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0f4758c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13438"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(disability_df_train['toxicity_binary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4df6b752",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = {x: tf_train_dataset[x] for x in bertweet_tokenizer.model_input_names}\n",
    "train_tf_dataset = tf.data.Dataset.from_tensor_slices((train_features, disability_df_train['toxicity_binary']))\n",
    "train_tf_dataset = train_tf_dataset.shuffle(len(tf_train_dataset)).batch(batch_size)\n",
    "\n",
    "val_features = {x: tf_val_dataset[x] for x in bertweet_tokenizer.model_input_names}\n",
    "val_tf_dataset = tf.data.Dataset.from_tensor_slices((val_features, disability_df_val['toxicity_binary']))\n",
    "val_tf_dataset = val_tf_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db321bf",
   "metadata": {},
   "source": [
    "## Transfer Learning by Freezing all BERTweet Layers, but training top layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2aabb6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460e0e289dfb4661bdbdd893e0ae6a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tf_model.h5:   0%|          | 0.00/740M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at vinai/bertweet-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at vinai/bertweet-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bertweet_model_base = TFRobertaModel.from_pretrained(\"vinai/bertweet-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b92eeec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn, pooler_layer_call_and_return_conditional_losses, embeddings_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models/assets\n"
     ]
    }
   ],
   "source": [
    "bertweet_model_base.save('saved_models/bertweet_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ea052474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "bertweet_model_loaded = tf.keras.models.load_model('saved_models/bertweet_base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261261fa",
   "metadata": {},
   "source": [
    "### Build a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f1accbed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"tf_roberta_model\" (type TFRobertaModel).\n\nCould not find matching concrete function to call loaded from the SavedModel. Got:\n  Positional arguments (14 total):\n    * {'attention_mask': <tf.Tensor 'input_ids_2:0' shape=(None, 128) dtype=int64>,\n 'input_ids': <tf.Tensor 'input_ids:0' shape=(None, 128) dtype=int64>,\n 'token_type_ids': <tf.Tensor 'input_ids_1:0' shape=(None, 128) dtype=int64>}\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * False\n  Keyword arguments: {}\n\n Expected these arguments to match one of the following 4 option(s):\n\nOption 1:\n  Positional arguments (14 total):\n    * {'attention_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name='attention_mask'),\n 'input_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_ids_input_ids')}\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * True\n  Keyword arguments: {}\n\nOption 2:\n  Positional arguments (14 total):\n    * {'attention_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name='attention_mask'),\n 'input_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_ids_input_ids')}\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * False\n  Keyword arguments: {}\n\nOption 3:\n  Positional arguments (14 total):\n    * {'attention_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_ids_attention_mask'),\n 'input_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_ids_input_ids')}\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * True\n  Keyword arguments: {}\n\nOption 4:\n  Positional arguments (14 total):\n    * {'attention_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_ids_attention_mask'),\n 'input_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_ids_input_ids')}\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * False\n  Keyword arguments: {}\n\nCall arguments received by layer \"tf_roberta_model\" (type TFRobertaModel):\n  • attention_mask={'input_ids': 'tf.Tensor(shape=(None, 128), dtype=int64)', 'token_type_ids': 'tf.Tensor(shape=(None, 128), dtype=int64)', 'attention_mask': 'tf.Tensor(shape=(None, 128), dtype=int64)'}\n  • token_type_ids=None\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • past_key_values=None\n  • use_cache=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 27\u001b[0m\n\u001b[1;32m     22\u001b[0m bertweet_inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: input_ids,\n\u001b[1;32m     23\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: token_type_ids,\n\u001b[1;32m     24\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: attention_mask}      \n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# bertweet_out = bertweet_model_loaded(bertweet_inputs, training=False)\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m bertweet_out \u001b[38;5;241m=\u001b[39m \u001b[43mbertweet_model_loaded\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbertweet_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# pooler_token = bertweet_out[1]\u001b[39;00m\n\u001b[1;32m     30\u001b[0m cls_token \u001b[38;5;241m=\u001b[39m bertweet_out[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m0\u001b[39m, :]\n",
      "File \u001b[0;32m~/anaconda3/envs/w266tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/w266tensorflow/lib/python3.10/site-packages/tensorflow/python/saved_model/function_deserialization.py:295\u001b[0m, in \u001b[0;36mrecreate_function.<locals>.restored_function_body\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m   positional, keyword \u001b[38;5;241m=\u001b[39m concrete_function\u001b[38;5;241m.\u001b[39mstructured_input_signature\n\u001b[1;32m    292\u001b[0m   signature_descriptions\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    293\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOption \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  Keyword arguments: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    294\u001b[0m           index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, _pretty_format_positional(positional), keyword))\n\u001b[0;32m--> 295\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find matching concrete function to call loaded from the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSavedModel. Got:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_pretty_format_positional(args)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  Keyword \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Expected these arguments to match one of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollowing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(saved_function\u001b[38;5;241m.\u001b[39mconcrete_functions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m option(s):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m10\u001b[39m))\u001b[38;5;241m.\u001b[39mjoin(signature_descriptions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"tf_roberta_model\" (type TFRobertaModel).\n\nCould not find matching concrete function to call loaded from the SavedModel. Got:\n  Positional arguments (14 total):\n    * {'attention_mask': <tf.Tensor 'input_ids_2:0' shape=(None, 128) dtype=int64>,\n 'input_ids': <tf.Tensor 'input_ids:0' shape=(None, 128) dtype=int64>,\n 'token_type_ids': <tf.Tensor 'input_ids_1:0' shape=(None, 128) dtype=int64>}\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * False\n  Keyword arguments: {}\n\n Expected these arguments to match one of the following 4 option(s):\n\nOption 1:\n  Positional arguments (14 total):\n    * {'attention_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name='attention_mask'),\n 'input_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_ids_input_ids')}\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * True\n  Keyword arguments: {}\n\nOption 2:\n  Positional arguments (14 total):\n    * {'attention_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name='attention_mask'),\n 'input_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_ids_input_ids')}\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * False\n  Keyword arguments: {}\n\nOption 3:\n  Positional arguments (14 total):\n    * {'attention_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_ids_attention_mask'),\n 'input_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_ids_input_ids')}\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * True\n  Keyword arguments: {}\n\nOption 4:\n  Positional arguments (14 total):\n    * {'attention_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_ids_attention_mask'),\n 'input_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_ids_input_ids')}\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * None\n    * False\n  Keyword arguments: {}\n\nCall arguments received by layer \"tf_roberta_model\" (type TFRobertaModel):\n  • attention_mask={'input_ids': 'tf.Tensor(shape=(None, 128), dtype=int64)', 'token_type_ids': 'tf.Tensor(shape=(None, 128), dtype=int64)', 'attention_mask': 'tf.Tensor(shape=(None, 128), dtype=int64)'}\n  • token_type_ids=None\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • past_key_values=None\n  • use_cache=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • training=False"
     ]
    }
   ],
   "source": [
    "# Build a simple classification model with BERTweet. Use the CLS token for classification purposes\n",
    "#     \"\"\"\n",
    "\n",
    "# Freeze all layers of pre-trained BERTweet model\n",
    "\n",
    "hidden_size = 16, \n",
    "dropout=0.3,\n",
    "# learning_rate=0.00005\n",
    "bertweet_model_loaded.trainable = False\n",
    "\n",
    "\n",
    "input_ids = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int64, name='input_ids_layer')\n",
    "# token_type_ids = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int64, name='token_type_ids_layer')\n",
    "attention_mask = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int64, name='attention_mask_layer')\n",
    "\n",
    "# bertweet_inputs = {'input_ids': input_ids,\n",
    "#                'token_type_ids': token_type_ids,\n",
    "#                'attention_mask': attention_mask}      \n",
    "\n",
    "# BERTweet only accepts input_ids and attention_mask as input, not token_type_ids\n",
    "\n",
    "bertweet_inputs = {'input_ids': input_ids,\n",
    "               'attention_mask': attention_mask}      \n",
    "\n",
    "# bertweet_out = bertweet_model_loaded(bertweet_inputs, training=False)\n",
    "bertweet_out = bertweet_model_loaded(bertweet_inputs)\n",
    "\n",
    "# pooler_token = bertweet_out[1]\n",
    "cls_token = bertweet_out[0][:, 0, :]\n",
    "\n",
    "hidden = tf.keras.layers.Dense(hidden_size, activation='relu', name='hidden_layer')(cls_token)\n",
    "\n",
    "\n",
    "hidden = tf.keras.layers.Dropout(dropout)(hidden)  \n",
    "\n",
    "\n",
    "classification = tf.keras.layers.Dense(1, activation='sigmoid',name='classification_layer')(hidden)\n",
    "\n",
    "classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=[classification])\n",
    "classification_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "35aac756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 126, 17, 11, 1156, 5199, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertweet_tokenizer(\"This is a test sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75809b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertweet_model_loaded(bertweet_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "55da27a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 126, 17, 11, 1156, 5199, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dd39ac0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7,), dtype=int32, numpy=array([   0,  126,   17,   11, 1156, 5199,    2], dtype=int32)>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.convert_to_tensor(test_sentence_tokenized['input_ids'],None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2cc5af23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 18:27:19.905427: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "test_sentence_tokenized = bertweet_tokenizer([\"This is a test sentence\", \"This is another test sentence\"])\n",
    "test_sentence_inputs = {'input_ids': tf.convert_to_tensor(test_sentence_tokenized['input_ids']),\n",
    "               'attention_mask': tf.convert_to_tensor(test_sentence_tokenized['attention_mask'])}\n",
    "test_sentence_output = bertweet_model_loaded(test_sentence_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "be30619f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 768), dtype=float32, numpy=\n",
       "array([[-0.06417338,  0.2017336 ,  0.1931608 , ..., -0.04359879,\n",
       "        -0.00547554,  0.00307681],\n",
       "       [-0.04666793,  0.2512347 ,  0.2019499 , ..., -0.02994854,\n",
       "         0.01557533,  0.01622131]], dtype=float32)>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_sentence_output['last_hidden_state'][:, 0, :]\n",
    "test_sentence_output['last_hidden_state'][:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ad3897d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 16, \n",
    "dropout=0.3,\n",
    "\n",
    "# Freeze all layers of pre-trained BERTweet model\n",
    "bertweet_model_loaded.trainable = False\n",
    "\n",
    "input_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_ids_input_ids')\n",
    "attention_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "# input_ids = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_ids_input_ids')\n",
    "# attention_mask = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "# BERTweet only accepts input_ids and attention_mask as input, not token_type_ids\n",
    "bertweet_inputs = {'input_ids': input_ids,\n",
    "               'attention_mask': attention_mask}\n",
    "\n",
    "# bertweet_out = bertweet_model_loaded(bertweet_inputs)\n",
    "bertweet_out = bertweet_model_loaded(bertweet_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fc9fc2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'last_hidden_state': <KerasTensor: shape=(None, None, 768) dtype=float32 (created by layer 'tf_roberta_model')>,\n",
       " 'pooler_output': <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_roberta_model')>}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertweet_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bd3da8ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(768,) dtype=float32 (created by layer 'tf.__operators__.getitem_12')>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertweet_out['last_hidden_state'][:, 0, :][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4abbb159",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# pooler_token = bertweet_out[1]\u001b[39;00m\n\u001b[1;32m      2\u001b[0m cls_token \u001b[38;5;241m=\u001b[39m bertweet_out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_hidden_state\u001b[39m\u001b[38;5;124m'\u001b[39m][:, \u001b[38;5;241m0\u001b[39m, :]\n\u001b[0;32m----> 4\u001b[0m hidden \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhidden_layer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m(cls_token)\n\u001b[1;32m      6\u001b[0m hidden \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDropout(dropout)(hidden)  \n\u001b[1;32m      9\u001b[0m classification \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m,name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification_layer\u001b[39m\u001b[38;5;124m'\u001b[39m)(hidden)\n",
      "File \u001b[0;32m~/anaconda3/envs/w266tensorflow/lib/python3.10/site-packages/keras/dtensor/utils.py:96\u001b[0m, in \u001b[0;36mallow_initializer_layout.<locals>._wrap_function\u001b[0;34m(layer_instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m layout:\n\u001b[1;32m     94\u001b[0m             layout_args[variable_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_layout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m layout\n\u001b[0;32m---> 96\u001b[0m \u001b[43minit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Inject the layout parameter after the invocation of __init__()\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layout_param_name, layout \u001b[38;5;129;01min\u001b[39;00m layout_args\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/anaconda3/envs/w266tensorflow/lib/python3.10/site-packages/keras/layers/core/dense.py:119\u001b[0m, in \u001b[0;36mDense.__init__\u001b[0;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;129m@utils\u001b[39m\u001b[38;5;241m.\u001b[39mallow_initializer_layout\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    116\u001b[0m ):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(activity_regularizer\u001b[38;5;241m=\u001b[39mactivity_regularizer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43munits\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(units, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m units\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    122\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived an invalid value for `units`, expected \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma positive integer. Received: units=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'tuple'"
     ]
    }
   ],
   "source": [
    "# pooler_token = bertweet_out[1]\n",
    "cls_token = bertweet_out['last_hidden_state'][:, 0, :]\n",
    "\n",
    "hidden = tf.keras.layers.Dense(hidden_size, activation='relu', name='hidden_layer')(cls_token)\n",
    "\n",
    "hidden = tf.keras.layers.Dropout(dropout)(hidden)  \n",
    "\n",
    "\n",
    "classification = tf.keras.layers.Dense(1, activation='sigmoid',name='classification_layer')(hidden)\n",
    "\n",
    "classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=[classification])\n",
    "classification_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527751fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010fdc3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4d61830",
   "metadata": {},
   "source": [
    "### Train the top layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e058ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                             loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), \n",
    "                             metrics=['accuracy','precision','recall','f1-score'])\n",
    "epochs = 10\n",
    "model.fit(train_tf_dataset, epochs=epochs, validation_data=val_tf_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467b0c75",
   "metadata": {},
   "source": [
    "### Do a round of fine-tuning of the entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e5d9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertweet_model_loaded.trainable = True\n",
    "classification_model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-5),  # Low learning rate\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), \n",
    "    metrics=['accuracy','precision','recall','f1-score']\n",
    ")\n",
    "\n",
    "epochs = 5\n",
    "model.fit(train_ds, epochs=epochs, validation_data=validation_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6125f8",
   "metadata": {},
   "source": [
    "## Fine-Tuning by Freezing only some Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efd7c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b24affa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9c83827",
   "metadata": {},
   "source": [
    "## Save weights/model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e7186e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "w266tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
