{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdb15e45",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "Data source: https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/data?select=all_data.csv\n",
    "\n",
    "In this notebook, we prepare the data downloaded from kaggle and export data subsets which we will feed into our models. The data preparation process includes data cleaning, extracting data subsets according to identity category, and splitting each identity subset into train, validation, and test sets via stratified sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef6f6faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d04d83",
   "metadata": {},
   "source": [
    "## Load the data:\n",
    "The kaggle competition corresponding to this dataset came with csv files for their own train and test subset. However, since the competition has ended, the `all_data.csv` file was released containing labels for both the train and test sets. Therefore, we'll be using the `all_data.csv` as our starting dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6108c6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_df = pd.read_csv('data/all_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c238e94",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d13e1f",
   "metadata": {},
   "source": [
    "EDA revealed that there were some rows with a missing value for `comment_text`. What does these rows look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ca35dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>split</th>\n",
       "      <th>created_date</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>...</th>\n",
       "      <th>white</th>\n",
       "      <th>asian</th>\n",
       "      <th>latino</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>446630</th>\n",
       "      <td>392337</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>2016-07-18 19:34:48.278774+00</td>\n",
       "      <td>13</td>\n",
       "      <td>392165.0</td>\n",
       "      <td>141670</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869804</th>\n",
       "      <td>872115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-01-21 02:04:30.064452+00</td>\n",
       "      <td>54</td>\n",
       "      <td>872109.0</td>\n",
       "      <td>163140</td>\n",
       "      <td>approved</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1556982</th>\n",
       "      <td>5971919</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-09-18 02:40:48.161601+00</td>\n",
       "      <td>13</td>\n",
       "      <td>5971615.0</td>\n",
       "      <td>378393</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567442</th>\n",
       "      <td>5353666</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-06-04 02:48:07.950238+00</td>\n",
       "      <td>13</td>\n",
       "      <td>5352881.0</td>\n",
       "      <td>340316</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id comment_text  split                   created_date   \n",
       "446630    392337          NaN  train  2016-07-18 19:34:48.278774+00  \\\n",
       "869804    872115          NaN  train  2017-01-21 02:04:30.064452+00   \n",
       "1556982  5971919          NaN  train  2017-09-18 02:40:48.161601+00   \n",
       "1567442  5353666          NaN  train  2017-06-04 02:48:07.950238+00   \n",
       "\n",
       "         publication_id  parent_id  article_id    rating  funny  wow  ...   \n",
       "446630               13   392165.0      141670  approved      0    0  ...  \\\n",
       "869804               54   872109.0      163140  approved      5    0  ...   \n",
       "1556982              13  5971615.0      378393  approved      0    0  ...   \n",
       "1567442              13  5352881.0      340316  approved      0    0  ...   \n",
       "\n",
       "         white  asian  latino  other_race_or_ethnicity  physical_disability   \n",
       "446630     NaN    NaN     NaN                      NaN                  NaN  \\\n",
       "869804     NaN    NaN     NaN                      NaN                  NaN   \n",
       "1556982    0.0    0.0     0.0                      0.0                  0.0   \n",
       "1567442    0.0    0.0     0.0                      0.0                  0.0   \n",
       "\n",
       "         intellectual_or_learning_disability  psychiatric_or_mental_illness   \n",
       "446630                                   NaN                            NaN  \\\n",
       "869804                                   NaN                            NaN   \n",
       "1556982                                  0.0                            0.0   \n",
       "1567442                                  0.0                            0.0   \n",
       "\n",
       "         other_disability  identity_annotator_count  toxicity_annotator_count  \n",
       "446630                NaN                         0                         4  \n",
       "869804                NaN                         0                         4  \n",
       "1556982               0.0                         4                         4  \n",
       "1567442               0.0                         4                         4  \n",
       "\n",
       "[4 rows x 46 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_df[pd.isna(all_data_df[\"comment_text\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa74e4d9",
   "metadata": {},
   "source": [
    "### Delete the row with the missing comments\n",
    "Since we'll have no input text to feed in for these rows, it will be unusable and therefore we'll remove them from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08e9a6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_df_cleansed = all_data_df.copy().drop(index=all_data_df[pd.isna(all_data_df['comment_text'])].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41494a9d",
   "metadata": {},
   "source": [
    "We can see that we now have a few less lines in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "010e104e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1999516, 46)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8482b9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1999512, 46)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_df_cleansed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e3d264",
   "metadata": {},
   "source": [
    "## Basic feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4287fb39",
   "metadata": {},
   "source": [
    "### Add `toxicity_binary` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d2a7926",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_df_cleansed['toxicity_binary'] = (all_data_df_cleansed['toxicity'] >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f9a069b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxicity</th>\n",
       "      <th>toxicity_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.373134</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.605263</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.815789</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.550000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999511</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999512</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999513</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999514</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999515</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1999512 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         toxicity  toxicity_binary\n",
       "0        0.373134                0\n",
       "1        0.605263                1\n",
       "2        0.666667                1\n",
       "3        0.815789                1\n",
       "4        0.550000                1\n",
       "...           ...              ...\n",
       "1999511  0.400000                0\n",
       "1999512  0.400000                0\n",
       "1999513  0.400000                0\n",
       "1999514  0.400000                0\n",
       "1999515  0.400000                0\n",
       "\n",
       "[1999512 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_df_cleansed[['toxicity','toxicity_binary']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc32ac5",
   "metadata": {},
   "source": [
    "Move the new column towards the front of the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dff6f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxicity_binary</th>\n",
       "      <th>split</th>\n",
       "      <th>created_date</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>...</th>\n",
       "      <th>white</th>\n",
       "      <th>asian</th>\n",
       "      <th>latino</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1083994</td>\n",
       "      <td>He got his money... now he lies in wait till a...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-03-06 15:21:53.675241+00</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>317120</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>650904</td>\n",
       "      <td>Mad dog will surely put the liberals in mental...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>2016-12-02 16:44:21.329535+00</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154086</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5902188</td>\n",
       "      <td>And Trump continues his lifelong cowardice by ...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-09-05 19:05:32.341360+00</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>374342</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7084460</td>\n",
       "      <td>\"while arresting a man for resisting arrest\".\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>2016-11-01 16:53:33.561631+00</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>149218</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5410943</td>\n",
       "      <td>Tucker and Paul are both total bad ass mofo's.</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-06-14 05:08:21.997315+00</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>344096</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                       comment_text   \n",
       "0  1083994  He got his money... now he lies in wait till a...  \\\n",
       "1   650904  Mad dog will surely put the liberals in mental...   \n",
       "2  5902188  And Trump continues his lifelong cowardice by ...   \n",
       "3  7084460  \"while arresting a man for resisting arrest\".\\...   \n",
       "4  5410943     Tucker and Paul are both total bad ass mofo's.   \n",
       "\n",
       "   toxicity_binary  split                   created_date  publication_id   \n",
       "0                0  train  2017-03-06 15:21:53.675241+00              21  \\\n",
       "1                1  train  2016-12-02 16:44:21.329535+00              21   \n",
       "2                1  train  2017-09-05 19:05:32.341360+00              55   \n",
       "3                1   test  2016-11-01 16:53:33.561631+00              13   \n",
       "4                1  train  2017-06-14 05:08:21.997315+00              21   \n",
       "\n",
       "   parent_id  article_id    rating  funny  ...  white  asian  latino   \n",
       "0        NaN      317120  approved      0  ...    NaN    NaN     NaN  \\\n",
       "1        NaN      154086  approved      0  ...    NaN    NaN     NaN   \n",
       "2        NaN      374342  approved      1  ...    NaN    NaN     NaN   \n",
       "3        NaN      149218  approved      0  ...    NaN    NaN     NaN   \n",
       "4        NaN      344096  approved      0  ...    NaN    NaN     NaN   \n",
       "\n",
       "   other_race_or_ethnicity  physical_disability   \n",
       "0                      NaN                  NaN  \\\n",
       "1                      NaN                  NaN   \n",
       "2                      NaN                  NaN   \n",
       "3                      NaN                  NaN   \n",
       "4                      NaN                  NaN   \n",
       "\n",
       "   intellectual_or_learning_disability  psychiatric_or_mental_illness   \n",
       "0                                  NaN                            NaN  \\\n",
       "1                                  NaN                            NaN   \n",
       "2                                  NaN                            NaN   \n",
       "3                                  NaN                            NaN   \n",
       "4                                  NaN                            NaN   \n",
       "\n",
       "   other_disability  identity_annotator_count  toxicity_annotator_count  \n",
       "0               NaN                         0                        67  \n",
       "1               NaN                         0                        76  \n",
       "2               NaN                         0                        63  \n",
       "3               NaN                         0                        76  \n",
       "4               NaN                         0                        80  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_cols = all_data_df_cleansed.columns.tolist()\n",
    "reordered_cols = orig_cols[:2] + orig_cols[-1:] + orig_cols[2:-1]\n",
    "all_data_df_cleansed = all_data_df_cleansed[reordered_cols]\n",
    "all_data_df_cleansed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeb98e1",
   "metadata": {},
   "source": [
    "## Drop the columns we won't be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ff8c97f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxicity_binary</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "      <th>transgender</th>\n",
       "      <th>other_gender</th>\n",
       "      <th>heterosexual</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>...</th>\n",
       "      <th>other_religion</th>\n",
       "      <th>black</th>\n",
       "      <th>white</th>\n",
       "      <th>asian</th>\n",
       "      <th>latino</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>other_disability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He got his money... now he lies in wait till a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.373134</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mad dog will surely put the liberals in mental...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>And Trump continues his lifelong cowardice by ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"while arresting a man for resisting arrest\".\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tucker and Paul are both total bad ass mofo's.</td>\n",
       "      <td>1</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxicity_binary   \n",
       "0  He got his money... now he lies in wait till a...                0  \\\n",
       "1  Mad dog will surely put the liberals in mental...                1   \n",
       "2  And Trump continues his lifelong cowardice by ...                1   \n",
       "3  \"while arresting a man for resisting arrest\".\\...                1   \n",
       "4     Tucker and Paul are both total bad ass mofo's.                1   \n",
       "\n",
       "   toxicity  male  female  transgender  other_gender  heterosexual   \n",
       "0  0.373134   NaN     NaN          NaN           NaN           NaN  \\\n",
       "1  0.605263   NaN     NaN          NaN           NaN           NaN   \n",
       "2  0.666667   NaN     NaN          NaN           NaN           NaN   \n",
       "3  0.815789   NaN     NaN          NaN           NaN           NaN   \n",
       "4  0.550000   NaN     NaN          NaN           NaN           NaN   \n",
       "\n",
       "   homosexual_gay_or_lesbian  bisexual  ...  other_religion  black  white   \n",
       "0                        NaN       NaN  ...             NaN    NaN    NaN  \\\n",
       "1                        NaN       NaN  ...             NaN    NaN    NaN   \n",
       "2                        NaN       NaN  ...             NaN    NaN    NaN   \n",
       "3                        NaN       NaN  ...             NaN    NaN    NaN   \n",
       "4                        NaN       NaN  ...             NaN    NaN    NaN   \n",
       "\n",
       "   asian  latino  other_race_or_ethnicity  physical_disability   \n",
       "0    NaN     NaN                      NaN                  NaN  \\\n",
       "1    NaN     NaN                      NaN                  NaN   \n",
       "2    NaN     NaN                      NaN                  NaN   \n",
       "3    NaN     NaN                      NaN                  NaN   \n",
       "4    NaN     NaN                      NaN                  NaN   \n",
       "\n",
       "   intellectual_or_learning_disability  psychiatric_or_mental_illness   \n",
       "0                                  NaN                            NaN  \\\n",
       "1                                  NaN                            NaN   \n",
       "2                                  NaN                            NaN   \n",
       "3                                  NaN                            NaN   \n",
       "4                                  NaN                            NaN   \n",
       "\n",
       "   other_disability  \n",
       "0               NaN  \n",
       "1               NaN  \n",
       "2               NaN  \n",
       "3               NaN  \n",
       "4               NaN  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_df_cleansed = all_data_df_cleansed.drop(columns=['id', 'split', 'created_date', 'publication_id',\n",
    "       'parent_id', 'article_id', 'rating', 'funny', 'wow', 'sad', 'likes',\n",
    "       'disagree', 'severe_toxicity', 'obscene', 'sexual_explicit',\n",
    "       'identity_attack', 'insult', 'threat', 'identity_annotator_count',\n",
    "       'toxicity_annotator_count'])\n",
    "all_data_df_cleansed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcbde89",
   "metadata": {},
   "source": [
    "# Prepare Disability Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6697880",
   "metadata": {},
   "source": [
    "## Create disability subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "793847d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "disability_df = all_data_df_cleansed[(all_data_df_cleansed[\"physical_disability\"] > 0) | \n",
    "           (all_data_df_cleansed[\"intellectual_or_learning_disability\"] > 0) | \n",
    "           (all_data_df_cleansed[\"psychiatric_or_mental_illness\"] > 0) | \n",
    "           (all_data_df_cleansed[\"other_disability\"] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57642faa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18665, 27)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disability_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e3cc7e",
   "metadata": {},
   "source": [
    "## Add disability subtype column\n",
    "We'll add a categorical feature that specifies which of the following disability subtypes each comment corresponds to:\n",
    "\n",
    "- `physical_disability`\n",
    "- `intellectual_or_learning_disability`\n",
    "- `psychiatric_or_mental_illness`\n",
    "- `other_disability`\n",
    "\n",
    "EDA revealed that some comments have nonzero values for more than one subtype above. Since the purpose of this comment is to facilitate stratified sampling, the disability subtype label for each comment will be the subtype corresponding to the largest value for that comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42adc80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxicity_binary</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "      <th>transgender</th>\n",
       "      <th>other_gender</th>\n",
       "      <th>heterosexual</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>...</th>\n",
       "      <th>black</th>\n",
       "      <th>white</th>\n",
       "      <th>asian</th>\n",
       "      <th>latino</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>disability_subtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7705</th>\n",
       "      <td>No sympathy for these two knuckleheads.</td>\n",
       "      <td>1</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>physical_disability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8073</th>\n",
       "      <td>Wow!\\nYour progressive psychosis has become ex...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>psychiatric_or_mental_illness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8115</th>\n",
       "      <td>Or.... maybe there IS chaos because the \"presi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.790323</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>psychiatric_or_mental_illness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8125</th>\n",
       "      <td>I'll take someone who's physically ill over on...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>psychiatric_or_mental_illness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8263</th>\n",
       "      <td>Mental Illness at work again, again, again, ag...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.842857</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>psychiatric_or_mental_illness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           comment_text  toxicity_binary   \n",
       "7705            No sympathy for these two knuckleheads.                1  \\\n",
       "8073  Wow!\\nYour progressive psychosis has become ex...                1   \n",
       "8115  Or.... maybe there IS chaos because the \"presi...                1   \n",
       "8125  I'll take someone who's physically ill over on...                0   \n",
       "8263  Mental Illness at work again, again, again, ag...                1   \n",
       "\n",
       "      toxicity  male  female  transgender  other_gender  heterosexual   \n",
       "7705  0.689655  0.00     0.0          0.0           0.0           0.0  \\\n",
       "8073  0.800000  0.00     0.0          0.0           0.0           0.0   \n",
       "8115  0.790323  0.00     0.0          0.0           0.0           0.0   \n",
       "8125  0.352941  0.00     0.0          0.0           0.0           0.0   \n",
       "8263  0.842857  0.25     1.0          0.0           0.0           0.0   \n",
       "\n",
       "      homosexual_gay_or_lesbian  bisexual  ...  black  white  asian  latino   \n",
       "7705                        0.0       0.0  ...    0.0    0.0    0.0     0.0  \\\n",
       "8073                        0.0       0.0  ...    0.0    0.0    0.0     0.0   \n",
       "8115                        0.0       0.0  ...    0.0    0.0    0.0     0.0   \n",
       "8125                        0.0       0.0  ...    0.0    0.0    0.0     0.0   \n",
       "8263                        0.0       0.0  ...    0.0    0.0    0.0     0.0   \n",
       "\n",
       "      other_race_or_ethnicity  physical_disability   \n",
       "7705                      0.0                 0.25  \\\n",
       "8073                      0.0                 0.00   \n",
       "8115                      0.0                 0.00   \n",
       "8125                      0.0                 0.75   \n",
       "8263                      0.0                 0.00   \n",
       "\n",
       "      intellectual_or_learning_disability  psychiatric_or_mental_illness   \n",
       "7705                                  0.0                            0.0  \\\n",
       "8073                                  0.0                            1.0   \n",
       "8115                                  0.0                            1.0   \n",
       "8125                                  0.0                            1.0   \n",
       "8263                                  0.0                            1.0   \n",
       "\n",
       "      other_disability             disability_subtype  \n",
       "7705               0.0            physical_disability  \n",
       "8073               0.0  psychiatric_or_mental_illness  \n",
       "8115               0.0  psychiatric_or_mental_illness  \n",
       "8125               0.0  psychiatric_or_mental_illness  \n",
       "8263               0.0  psychiatric_or_mental_illness  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disability_subtypes_df = disability_df[['physical_disability','intellectual_or_learning_disability','psychiatric_or_mental_illness','other_disability']]\n",
    "disability_df = disability_df.assign(disability_subtype=disability_subtypes_df.idxmax(axis=1))\n",
    "disability_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00734fce",
   "metadata": {},
   "source": [
    "### Prepare data splits for disability\n",
    "**Overview:** We'll split the disability subset into 70% train, 10% validation, and 20% test sets. Comments may have subtle differences due to disability subtypes (e.g. comments about physical disability may be different than comments about intellectual/learning disability). Therefore we'll need to do stratified sampling on the disability subtypes such that for each dataset split, the ratio for each disability subtype will be around the same.\n",
    "\n",
    "**Disability+Non-Disability Interweaving Technique:** Every split should have stratified sampling. We'll divide the disabilty subset into 3 sets stratified on disability subtype, and divide a non-disability subset (e.g. gender) into 3 sets stratified on its corresponding subtype. Then we'll alternate between each disability split and non-disability split when fine-tuning (i.e. train on disability split 1, then train on gender split 1, then train on disability split 2, then train on gender split 2, etc.). For the alternating technique, we'll need 3 training sets and 3 validation sets for disability, and all sets will need to be stratified on the disability subtype.\n",
    "\n",
    "**Splitting Method:** To prepare the splits as previously described, we'll use the train_test_split() method and since it only creates two splits, we'll take the following steps to create all of the dataset splits:\n",
    "\n",
    "1. Split into group1: 80% for train and validation, and group2: 20% for test.\n",
    "1. No need to further split the test set, so leave it alone.\n",
    "1. Take the set from step 1 that combines train and validation, and split it into group1: 2/3 and group2: 1/3.\n",
    "1. Take the set from step 3 that was 2/3 and split it into half.\n",
    "1. Now we have 3 equal splits from step 3 and step 4.\n",
    "1. For each of the 3 splits, create a train and validation split. Since we want the overall ratio to be 70% train and 10% validation, the ratio for train here should be (1-1/7) and for validation it should be 1/7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91887ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into 80% combined for train and val, and 20% test\n",
    "disability_combined_df, disability_test_df = train_test_split(disability_df,\n",
    "                                       test_size=0.2,\n",
    "                                       random_state=266, stratify=disability_df['disability_subtype'])\n",
    "\n",
    "# Split the 80% train and val into: 2/3 and 1/3\n",
    "disability_combined_split12_df, disability_combined_split3_df = train_test_split(disability_combined_df,\n",
    "                                       test_size=1/3,\n",
    "                                       random_state=266, stratify=disability_combined_df['disability_subtype'])\n",
    "\n",
    "# Split the 2/3 train and val into half\n",
    "disability_combined_split1_df, disability_combined_split2_df = train_test_split(disability_combined_split12_df,\n",
    "                                       test_size=0.5,\n",
    "                                       random_state=266, stratify=disability_combined_split12_df['disability_subtype'])\n",
    "\n",
    "# Create (1-1/7) train and 1/7 val for Split #1\n",
    "# We want 70% train and 10% val\n",
    "disability_split1_train_df, disability_split1_val_df = train_test_split(disability_combined_split1_df,\n",
    "                                       test_size=1/7,\n",
    "                                       random_state=266, stratify=disability_combined_split1_df['disability_subtype'])\n",
    "\n",
    "# Create (1-1/7) train and 1/7 val for Split #2\n",
    "# We want 70% train and 10% val\n",
    "disability_split2_train_df, disability_split2_val_df = train_test_split(disability_combined_split2_df,\n",
    "                                       test_size=1/7,\n",
    "                                       random_state=266, stratify=disability_combined_split2_df['disability_subtype'])\n",
    "\n",
    "# Create (1-1/7) train and 1/7 val for Split #3\n",
    "# We want 70% train and 10% val\n",
    "disability_split3_train_df, disability_split3_val_df = train_test_split(disability_combined_split3_df,\n",
    "                                       test_size=1/7,\n",
    "                                       random_state=266, stratify=disability_combined_split3_df['disability_subtype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3aca6bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "How big is each disability split?\n",
      "\n",
      "len(disability_split1_train_df):  4266\n",
      "len(disability_split2_train_df):  4266\n",
      "len(disability_split3_train_df):  4266\n",
      "len(disability_split1_val_df):  711\n",
      "len(disability_split2_val_df):  711\n",
      "len(disability_split3_val_df):  712\n",
      "total disability train:  12798\n",
      "total disability val:  2134\n",
      "len(disability_test_df):  3733\n"
     ]
    }
   ],
   "source": [
    "print('\\nHow big is each disability split?\\n')\n",
    "print('len(disability_split1_train_df): ', len(disability_split1_train_df))\n",
    "print('len(disability_split2_train_df): ', len(disability_split2_train_df))\n",
    "print('len(disability_split3_train_df): ', len(disability_split3_train_df))\n",
    "\n",
    "print('len(disability_split1_val_df): ', len(disability_split1_val_df))\n",
    "print('len(disability_split2_val_df): ', len(disability_split2_val_df))\n",
    "print('len(disability_split3_val_df): ', len(disability_split3_val_df))\n",
    "\n",
    "print('total disability train: ', len(disability_split1_train_df)+len(disability_split2_train_df)+len(disability_split3_train_df))\n",
    "print('total disability val: ', len(disability_split1_val_df)+len(disability_split2_val_df)+len(disability_split3_val_df))\n",
    "print('len(disability_test_df): ', len(disability_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63db86e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "disability_train_df = pd.concat([disability_split1_train_df, disability_split2_train_df, disability_split3_train_df], axis=0)\n",
    "disability_val_df = pd.concat([disability_split1_val_df, disability_split2_val_df, disability_split3_val_df], axis=0)\n",
    "disability_full_df = pd.concat([disability_train_df, disability_val_df, disability_test_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72b20dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(disability_train_df):  12798\n",
      "len(disability_val_df):  2134\n",
      "len(disability_test_df):  3733\n",
      "len(disability_full_df):  18665\n"
     ]
    }
   ],
   "source": [
    "print('len(disability_train_df): ', len(disability_train_df))\n",
    "print('len(disability_val_df): ', len(disability_val_df))\n",
    "print('len(disability_test_df): ', len(disability_test_df))\n",
    "print('len(disability_full_df): ', len(disability_full_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0663082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stratified Sampling Sanity Check for Disability\n",
      "\n",
      "Split 1 disability Train\n",
      "=====================\n",
      "phys:\t 0.15002344116268168\n",
      "intel:\t 0.11509610876699485\n",
      "psych:\t 0.5928270042194093\n",
      "other_disability:\t 0.1420534458509142\n",
      "\n",
      "Split 1 disability Val\n",
      "=====================\n",
      "phys:\t 0.15049226441631505\n",
      "intel:\t 0.11533052039381153\n",
      "psych:\t 0.5921237693389592\n",
      "other_disability:\t 0.1420534458509142\n"
     ]
    }
   ],
   "source": [
    "print('\\nStratified Sampling Sanity Check for Disability')\n",
    "disability_train_phys = (disability_split1_train_df['disability_subtype']=='physical_disability').astype(int).sum()\n",
    "disability_train_intel = (disability_split1_train_df['disability_subtype']=='intellectual_or_learning_disability').astype(int).sum()\n",
    "disability_train_psych = (disability_split1_train_df['disability_subtype']=='psychiatric_or_mental_illness').astype(int).sum()\n",
    "disability_train_other = (disability_split1_train_df['disability_subtype']=='other_disability').astype(int).sum()\n",
    "disability_train_total = len(disability_split1_train_df['disability_subtype'])\n",
    "print('\\nSplit 1 disability Train')\n",
    "print('=====================')\n",
    "print('phys:\\t', disability_train_phys/disability_train_total)\n",
    "print('intel:\\t', disability_train_intel/disability_train_total)\n",
    "print('psych:\\t', disability_train_psych/disability_train_total)\n",
    "print('other_disability:\\t', disability_train_other/disability_train_total)\n",
    "\n",
    "disability_val_phys = (disability_split1_val_df['disability_subtype']=='physical_disability').astype(int).sum()\n",
    "disability_val_intel = (disability_split1_val_df['disability_subtype']=='intellectual_or_learning_disability').astype(int).sum()\n",
    "disability_val_psych = (disability_split1_val_df['disability_subtype']=='psychiatric_or_mental_illness').astype(int).sum()\n",
    "disability_val_other = (disability_split1_val_df['disability_subtype']=='other_disability').astype(int).sum()\n",
    "disability_val_total = len(disability_split1_val_df['disability_subtype'])\n",
    "print('\\nSplit 1 disability Val')\n",
    "print('=====================')\n",
    "print('phys:\\t', disability_val_phys/disability_val_total)\n",
    "print('intel:\\t', disability_val_intel/disability_val_total)\n",
    "print('psych:\\t', disability_val_psych/disability_val_total)\n",
    "print('other_disability:\\t', disability_val_other/disability_val_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a261c7ac",
   "metadata": {},
   "source": [
    "### Export disability split datasets into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7386d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "disability_df.to_csv('data/disability-dataset-full.csv')\n",
    "disability_train_df.to_csv('data/disability-dataset-train.csv')\n",
    "disability_val_df.to_csv('data/disability-dataset-val.csv')\n",
    "disability_test_df.to_csv('data/disability-dataset-test.csv')\n",
    "\n",
    "disability_split1_train_df.to_csv('data/disability-dataset-split1-train.csv')\n",
    "disability_split2_train_df.to_csv('data/disability-dataset-split2-train.csv')\n",
    "disability_split3_train_df.to_csv('data/disability-dataset-split3-train.csv')\n",
    "\n",
    "disability_split1_val_df.to_csv('data/disability-dataset-split1-val.csv')\n",
    "disability_split2_val_df.to_csv('data/disability-dataset-split2-val.csv')\n",
    "disability_split3_val_df.to_csv('data/disability-dataset-split3-val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80f528c",
   "metadata": {},
   "source": [
    "### Address Data Imbalance Between Disability and Non-Disability Subsets\n",
    "All of the non-disability identities have many more records than the disability subset. For the interweaving technique, we'll want the disability and non-disability subset to be balanced. That is, we don't want whatever is learned from the disability subset to be overpowered by the non-disability subset due training on more non-disability examples. Therefore, we'll do stratified undersampling of the non-disability subsets such that they're around the same size as the disability subset. (Since disability is our focus and we're only augmenting other identity groups to help with predicting disability-related comments, we're okay with discarding data for other identity groups).\n",
    "\n",
    "Capture number of disability samples in disability train, val, and test set to be used in the undersampling for non-disability identities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98b32578",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_disability_train_samples = len(disability_train_df)\n",
    "num_disability_val_samples = len(disability_val_df)\n",
    "num_disability_test_samples = len(disability_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e432b8",
   "metadata": {},
   "source": [
    "# Prepare non-disability subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d09f1aa",
   "metadata": {},
   "source": [
    "## Gender subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c3b1ed",
   "metadata": {},
   "source": [
    "Create gender subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e653523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137722, 27)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_df = all_data_df_cleansed[(all_data_df_cleansed['male'] > 0) | \n",
    "           (all_data_df_cleansed['female'] > 0) | \n",
    "           (all_data_df_cleansed['transgender'] > 0) | \n",
    "           (all_data_df_cleansed['other_gender'] > 0)]\n",
    "gender_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d92306f",
   "metadata": {},
   "source": [
    "Add gender subtype column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c17c18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxicity_binary</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "      <th>transgender</th>\n",
       "      <th>other_gender</th>\n",
       "      <th>heterosexual</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>...</th>\n",
       "      <th>black</th>\n",
       "      <th>white</th>\n",
       "      <th>asian</th>\n",
       "      <th>latino</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>gender_subtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7681</th>\n",
       "      <td>Blame men.  There's always an excuse to blame ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7682</th>\n",
       "      <td>And the woman exposing herself saying grab thi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.728571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7691</th>\n",
       "      <td>Are you a Pilgrim?\\nWhy arn't you growing your...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7699</th>\n",
       "      <td>No, he was accused of being a racist white man.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7709</th>\n",
       "      <td>How do we fight agaisnt women who use sexual f...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           comment_text  toxicity_binary   \n",
       "7681  Blame men.  There's always an excuse to blame ...                1  \\\n",
       "7682  And the woman exposing herself saying grab thi...                1   \n",
       "7691  Are you a Pilgrim?\\nWhy arn't you growing your...                1   \n",
       "7699    No, he was accused of being a racist white man.                0   \n",
       "7709  How do we fight agaisnt women who use sexual f...                1   \n",
       "\n",
       "      toxicity  male  female  transgender  other_gender  heterosexual   \n",
       "7681  0.545455   1.0     1.0          0.0           0.0           0.0  \\\n",
       "7682  0.728571   0.0     1.0          0.0           0.0           0.0   \n",
       "7691  0.800000   1.0     0.0          0.0           0.0           0.0   \n",
       "7699  0.363636   1.0     0.0          0.0           0.0           0.0   \n",
       "7709  0.800000   1.0     1.0          0.0           0.0           0.0   \n",
       "\n",
       "      homosexual_gay_or_lesbian  bisexual  ...  black  white  asian  latino   \n",
       "7681                        0.0       0.0  ...    0.0    0.0    0.0     0.0  \\\n",
       "7682                        0.0       0.0  ...    0.0    0.0    0.0     0.0   \n",
       "7691                        0.0       0.0  ...    0.0    1.0    0.0     0.0   \n",
       "7699                        0.0       0.0  ...    0.0    1.0    0.0     0.0   \n",
       "7709                        0.0       0.0  ...    0.0    0.0    0.0     0.0   \n",
       "\n",
       "      other_race_or_ethnicity  physical_disability   \n",
       "7681                      0.0                  0.0  \\\n",
       "7682                      0.0                  0.0   \n",
       "7691                      0.0                  0.0   \n",
       "7699                      0.0                  0.0   \n",
       "7709                      0.0                  0.0   \n",
       "\n",
       "      intellectual_or_learning_disability  psychiatric_or_mental_illness   \n",
       "7681                                  0.0                            0.0  \\\n",
       "7682                                  0.0                            0.0   \n",
       "7691                                  0.0                            0.0   \n",
       "7699                                  0.0                            0.0   \n",
       "7709                                  0.0                            0.0   \n",
       "\n",
       "      other_disability  gender_subtype  \n",
       "7681               0.0            male  \n",
       "7682               0.0          female  \n",
       "7691               0.0            male  \n",
       "7699               0.0            male  \n",
       "7709               0.0            male  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_subtypes_df = gender_df[['male','female','transgender','other_gender']]\n",
    "gender_df = gender_df.assign(gender_subtype=gender_subtypes_df.idxmax(axis=1))\n",
    "gender_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c3d6db",
   "metadata": {},
   "source": [
    "### Prepare data splits for gender\n",
    "**Overview:** We want the gender subset to have the following ratio: 70% train, 10% val, and 20% test.\n",
    "\n",
    "**Disability+Gender Interweaving Technique:** Each split should have stratified sampling. We'll divide the disabilty subset into 3 sets stratified on disability subtype, and divide the gender disability subset into 3 sets stratified on gender subtype. Then we'll alternate between each disability split and gender split when fine-tuning (i.e. train on disability split 1, then train on gender split 1, then train on disability split 2, then train on gender split 2, etc.). For the alternating technique, we'll need 3 training and validation sets for gender, and all sets will need to be stratified on the gender subtype.\n",
    "\n",
    "**Splitting Method:** To prepare the splits as previously described, we'll use the train_test_split() method and since it only creates two splits, we'll take the following steps to create all of the dataset splits:\n",
    "\n",
    "1. Split into group1: 80% for train and validation, and group2: 20% for test.\n",
    "1. No need to further split the test set, so leave it alone.\n",
    "1. Take the set from step 1 that combines train and validation, and split it into group1: 2/3 and group2: 1/3.\n",
    "1. Take the set from step 3 that was 2/3 and split it into half.\n",
    "1. Now we have 3 equal splits from step 3 and step 4.\n",
    "1. For each of the 3 splits, create a train and validation split. Since we want the overall ratio to be 70% train and 10% validation, the ratio for train here should be (1-1/7) and for validation it should be 1/7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "320177a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into 80% combined for train and val, and 20% test\n",
    "gender_combined_df, gender_test_df = train_test_split(gender_df,\n",
    "                                       train_size=num_disability_train_samples+num_disability_val_samples,\n",
    "                                       test_size=num_disability_test_samples,\n",
    "                                       random_state=266, stratify=gender_df['gender_subtype'])\n",
    "\n",
    "# Split the 80% train and val into: 2/3 and 1/3\n",
    "gender_combined_split12_df, gender_combined_split3_df = train_test_split(gender_combined_df,\n",
    "                                       test_size=1/3,\n",
    "                                       random_state=266, stratify=gender_combined_df['gender_subtype'])\n",
    "\n",
    "# Split the 2/3 train and val into half\n",
    "gender_combined_split1_df, gender_combined_split2_df = train_test_split(gender_combined_split12_df,\n",
    "                                       test_size=0.5,\n",
    "                                       random_state=266, stratify=gender_combined_split12_df['gender_subtype'])\n",
    "\n",
    "# Create (1-1/7) train and 1/7 val for Split #1\n",
    "# We want 70% train and 10% val\n",
    "gender_split1_train_df, gender_split1_val_df = train_test_split(gender_combined_split1_df,\n",
    "                                       test_size=1/7,\n",
    "                                       random_state=266, stratify=gender_combined_split1_df['gender_subtype'])\n",
    "\n",
    "# Create (1-1/7) train and 1/7 val for Split #2\n",
    "# We want 70% train and 10% val\n",
    "gender_split2_train_df, gender_split2_val_df = train_test_split(gender_combined_split2_df,\n",
    "                                       test_size=1/7,\n",
    "                                       random_state=266, stratify=gender_combined_split2_df['gender_subtype'])\n",
    "\n",
    "# Create (1-1/7) train and 1/7 val for Split #3\n",
    "# We want 70% train and 10% val\n",
    "gender_split3_train_df, gender_split3_val_df = train_test_split(gender_combined_split3_df,\n",
    "                                       test_size=1/7,\n",
    "                                       random_state=266, stratify=gender_combined_split3_df['gender_subtype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4af086f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "How big is each split for gender?\n",
      "\n",
      "len(gender_split1_train_df):  4266\n",
      "len(gender_split2_train_df):  4266\n",
      "len(gender_split3_train_df):  4266\n",
      "len(gender_split1_val_df):  711\n",
      "len(gender_split2_val_df):  711\n",
      "len(gender_split3_val_df):  712\n",
      "total gender train:  12798\n",
      "total gender val:  2134\n",
      "len(gender_test_df):  3733\n"
     ]
    }
   ],
   "source": [
    "print('\\nHow big is each split for gender?\\n')\n",
    "print('len(gender_split1_train_df): ', len(gender_split1_train_df))\n",
    "print('len(gender_split2_train_df): ', len(gender_split2_train_df))\n",
    "print('len(gender_split3_train_df): ', len(gender_split3_train_df))\n",
    "\n",
    "print('len(gender_split1_val_df): ', len(gender_split1_val_df))\n",
    "print('len(gender_split2_val_df): ', len(gender_split2_val_df))\n",
    "print('len(gender_split3_val_df): ', len(gender_split3_val_df))\n",
    "\n",
    "print('total gender train: ', len(gender_split1_train_df)+len(gender_split2_train_df)+len(gender_split3_train_df))\n",
    "print('total gender val: ', len(gender_split1_val_df)+len(gender_split2_val_df)+len(gender_split3_val_df))\n",
    "print('len(gender_test_df): ', len(gender_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c82e5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_train_df = pd.concat([gender_split1_train_df, gender_split2_train_df, gender_split3_train_df], axis=0)\n",
    "gender_val_df = pd.concat([gender_split1_val_df, gender_split2_val_df, gender_split3_val_df], axis=0)\n",
    "gender_full_df = pd.concat([gender_train_df, gender_val_df, gender_test_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a32bda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(gender_train_df):  12798\n",
      "len(gender_val_df):  2134\n",
      "len(gender_test_df):  3733\n",
      "len(gender_full_df):  18665\n"
     ]
    }
   ],
   "source": [
    "print('len(gender_train_df): ', len(gender_train_df))\n",
    "print('len(gender_val_df): ', len(gender_val_df))\n",
    "print('len(gender_test_df): ', len(gender_test_df))\n",
    "print('len(gender_full_df): ', len(gender_full_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f49ca7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stratified Sampling Sanity Check for Gender\n",
      "\n",
      "Split 1 Gender Train\n",
      "=====================\n",
      "male:\t 0.5307079231129864\n",
      "female:\t 0.42522269104547583\n",
      "transgender:\t 0.03164556962025317\n",
      "other_gender:\t 0.012423816221284576\n",
      "\n",
      "Split 1 Gender Val\n",
      "=====================\n",
      "male:\t 0.530239099859353\n",
      "female:\t 0.42616033755274263\n",
      "transgender:\t 0.030942334739803096\n",
      "other_gender:\t 0.012658227848101266\n"
     ]
    }
   ],
   "source": [
    "print('\\nStratified Sampling Sanity Check for Gender')\n",
    "gender_train_male = (gender_split1_train_df['gender_subtype']=='male').astype(int).sum()\n",
    "gender_train_female = (gender_split1_train_df['gender_subtype']=='female').astype(int).sum()\n",
    "gender_train_trans = (gender_split1_train_df['gender_subtype']=='transgender').astype(int).sum()\n",
    "gender_train_other = (gender_split1_train_df['gender_subtype']=='other_gender').astype(int).sum()\n",
    "gender_train_total = len(gender_split1_train_df['gender_subtype'])\n",
    "print('\\nSplit 1 Gender Train')\n",
    "print('=====================')\n",
    "print('male:\\t', gender_train_male/gender_train_total)\n",
    "print('female:\\t', gender_train_female/gender_train_total)\n",
    "print('transgender:\\t', gender_train_trans/gender_train_total)\n",
    "print('other_gender:\\t', gender_train_other/gender_train_total)\n",
    "\n",
    "gender_val_male = (gender_split1_val_df['gender_subtype']=='male').astype(int).sum()\n",
    "gender_val_female = (gender_split1_val_df['gender_subtype']=='female').astype(int).sum()\n",
    "gender_val_trans = (gender_split1_val_df['gender_subtype']=='transgender').astype(int).sum()\n",
    "gender_val_other = (gender_split1_val_df['gender_subtype']=='other_gender').astype(int).sum()\n",
    "gender_val_total = len(gender_split1_val_df['gender_subtype'])\n",
    "print('\\nSplit 1 Gender Val')\n",
    "print('=====================')\n",
    "print('male:\\t', gender_val_male/gender_val_total)\n",
    "print('female:\\t', gender_val_female/gender_val_total)\n",
    "print('transgender:\\t', gender_val_trans/gender_val_total)\n",
    "print('other_gender:\\t', gender_val_other/gender_val_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35f09e7",
   "metadata": {},
   "source": [
    "### Export gender split datasets into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96226409",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_full_df.to_csv('data/gender-dataset-full.csv')\n",
    "gender_train_df.to_csv('data/gender-dataset-train.csv')\n",
    "gender_val_df.to_csv('data/gender-dataset-val.csv')\n",
    "gender_test_df.to_csv('data/gender-dataset-test.csv')\n",
    "\n",
    "gender_split1_train_df.to_csv('data/gender-dataset-split1-train.csv')\n",
    "gender_split2_train_df.to_csv('data/gender-dataset-split2-train.csv')\n",
    "gender_split3_train_df.to_csv('data/gender-dataset-split3-train.csv')\n",
    "\n",
    "gender_split1_val_df.to_csv('data/gender-dataset-split1-val.csv')\n",
    "gender_split2_val_df.to_csv('data/gender-dataset-split2-val.csv')\n",
    "gender_split3_val_df.to_csv('data/gender-dataset-split3-val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba40fe1e",
   "metadata": {},
   "source": [
    "## Sexual orientation subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce741348",
   "metadata": {},
   "source": [
    "Create sexual orientation subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6911615e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22649, 27)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sexual_orientation_df = all_data_df_cleansed[(all_data_df_cleansed['heterosexual'] > 0) | \n",
    "           (all_data_df_cleansed['homosexual_gay_or_lesbian'] > 0) | \n",
    "           (all_data_df_cleansed['bisexual'] > 0) | \n",
    "           (all_data_df_cleansed['other_sexual_orientation'] > 0)]\n",
    "sexual_orientation_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c9310a",
   "metadata": {},
   "source": [
    "Add sexual orientation subtype column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "341e5002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxicity_binary</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "      <th>transgender</th>\n",
       "      <th>other_gender</th>\n",
       "      <th>heterosexual</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>...</th>\n",
       "      <th>black</th>\n",
       "      <th>white</th>\n",
       "      <th>asian</th>\n",
       "      <th>latino</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>sexual_orientation_subtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7695</th>\n",
       "      <td>Ridiculous, indeed. Although Rome does seem to...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>homosexual_gay_or_lesbian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7716</th>\n",
       "      <td>It took them long enough. And it goes against ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>homosexual_gay_or_lesbian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7746</th>\n",
       "      <td>Well now Murray can simply go for the bisexual...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bisexual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7890</th>\n",
       "      <td>He probably thoughthimself gay when he did not...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.453333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>homosexual_gay_or_lesbian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7894</th>\n",
       "      <td>Sodomy isn't exclusive to homosexuals. I can s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>homosexual_gay_or_lesbian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           comment_text  toxicity_binary   \n",
       "7695  Ridiculous, indeed. Although Rome does seem to...                1  \\\n",
       "7716  It took them long enough. And it goes against ...                0   \n",
       "7746  Well now Murray can simply go for the bisexual...                1   \n",
       "7890  He probably thoughthimself gay when he did not...                0   \n",
       "7894  Sodomy isn't exclusive to homosexuals. I can s...                0   \n",
       "\n",
       "      toxicity  male  female  transgender  other_gender  heterosexual   \n",
       "7695  0.857143   0.0     0.0          0.0           0.0           0.0  \\\n",
       "7716  0.181818   0.0     0.0          0.0           0.0           0.0   \n",
       "7746  0.800000   1.0     0.0          0.0           0.0           0.0   \n",
       "7890  0.453333   0.0     1.0          0.0           0.0           0.0   \n",
       "7894  0.437500   0.0     0.0          0.0           0.0           0.0   \n",
       "\n",
       "      homosexual_gay_or_lesbian  bisexual  ...  black  white  asian  latino   \n",
       "7695                        1.0       0.0  ...    0.0    0.0    0.0     0.0  \\\n",
       "7716                        1.0       0.0  ...    0.0    0.0    0.0     0.0   \n",
       "7746                        0.0       1.0  ...    0.0    0.0    0.0     0.0   \n",
       "7890                        1.0       0.0  ...    0.0    0.0    0.0     0.0   \n",
       "7894                        1.0       0.0  ...    0.0    0.0    0.0     0.0   \n",
       "\n",
       "      other_race_or_ethnicity  physical_disability   \n",
       "7695                      0.0                  0.0  \\\n",
       "7716                      0.0                  0.0   \n",
       "7746                      0.0                  0.0   \n",
       "7890                      0.0                  0.0   \n",
       "7894                      0.0                  0.0   \n",
       "\n",
       "      intellectual_or_learning_disability  psychiatric_or_mental_illness   \n",
       "7695                                  0.0                            0.0  \\\n",
       "7716                                  0.0                            0.0   \n",
       "7746                                  0.0                            0.0   \n",
       "7890                                  0.0                            0.0   \n",
       "7894                                  0.0                            0.0   \n",
       "\n",
       "      other_disability  sexual_orientation_subtype  \n",
       "7695               0.0   homosexual_gay_or_lesbian  \n",
       "7716               0.0   homosexual_gay_or_lesbian  \n",
       "7746               0.0                    bisexual  \n",
       "7890               0.0   homosexual_gay_or_lesbian  \n",
       "7894               0.0   homosexual_gay_or_lesbian  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sexual_orientation_subtypes_df = sexual_orientation_df[['heterosexual','homosexual_gay_or_lesbian','bisexual','other_sexual_orientation']]\n",
    "sexual_orientation_df = sexual_orientation_df.assign(sexual_orientation_subtype=sexual_orientation_subtypes_df.idxmax(axis=1))\n",
    "sexual_orientation_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8020f8",
   "metadata": {},
   "source": [
    "### Prepare data splits for sexual orientation\n",
    "**Overview:** We want the sexual orientation subset to have the following ratio: 70% train, 10% val, and 20% test.\n",
    "\n",
    "**Disability+Sexual Orientation Interweaving Technique:** Each split should have stratified sampling. We'll divide the disabilty subset into 3 sets stratified on disability subtype, and divide the sexual orientation disability subset into 3 sets stratified on sexual orientation subtype. Then we'll alternate between each disability split and sexual orientation split when fine-tuning (i.e. train on disability split 1, then train on sexual orientation split 1, then train on disability split 2, then train on sexual orientation split 2, etc.). For the alternating technique, we'll need 3 training sets for sexual orientation and 3 validation sets for sexual orientation, and all sets will need to be stratified on the sexual orientation subtype.\n",
    "\n",
    "**Splitting Method:** To prepare the splits as previously described, we'll use the train_test_split() method and since it only creates two splits, we'll take the following steps to create all of the dataset splits:\n",
    "\n",
    "1. Split into group1: 80% for train and validation, and group2: 20% for test.\n",
    "1. No need to further split the test set, so leave it alone.\n",
    "1. Take the set from step 1 that combines train and validation, and split it into group1: 2/3 and group2: 1/3.\n",
    "1. Take the set from step 3 that was 2/3 and split it into half.\n",
    "1. Now we have 3 equal splits from step 3 and step 4.\n",
    "1. For each of the 3 splits, create a train and validation split. Since we want the overall ratio to be 70% train and 10% validation, the ratio for train here should be (1-1/7) and for validation it should be 1/7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4244b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into 80% combined for train and val, and 20% test\n",
    "sexual_orientation_combined_df, sexual_orientation_test_df = train_test_split(sexual_orientation_df,\n",
    "                                       train_size=num_disability_train_samples+num_disability_val_samples,\n",
    "                                       test_size=num_disability_test_samples,\n",
    "                                       random_state=266, stratify=sexual_orientation_df['sexual_orientation_subtype'])\n",
    "\n",
    "# Split the 80% train and val into: 2/3 and 1/3\n",
    "sexual_orientation_combined_split12_df, sexual_orientation_combined_split3_df = train_test_split(sexual_orientation_combined_df,\n",
    "                                       test_size=1/3,\n",
    "                                       random_state=266, stratify=sexual_orientation_combined_df['sexual_orientation_subtype'])\n",
    "\n",
    "# Split the 2/3 train and val into half\n",
    "sexual_orientation_combined_split1_df, sexual_orientation_combined_split2_df = train_test_split(sexual_orientation_combined_split12_df,\n",
    "                                       test_size=0.5,\n",
    "                                       random_state=266, stratify=sexual_orientation_combined_split12_df['sexual_orientation_subtype'])\n",
    "\n",
    "# Create (1-1/7) train and 1/7 val for Split #1\n",
    "# We want 70% train and 10% val\n",
    "sexual_orientation_split1_train_df, sexual_orientation_split1_val_df = train_test_split(sexual_orientation_combined_split1_df,\n",
    "                                       test_size=1/7,\n",
    "                                       random_state=266, stratify=sexual_orientation_combined_split1_df['sexual_orientation_subtype'])\n",
    "\n",
    "# Create (1-1/7) train and 1/7 val for Split #2\n",
    "# We want 70% train and 10% val\n",
    "sexual_orientation_split2_train_df, sexual_orientation_split2_val_df = train_test_split(sexual_orientation_combined_split2_df,\n",
    "                                       test_size=1/7,\n",
    "                                       random_state=266, stratify=sexual_orientation_combined_split2_df['sexual_orientation_subtype'])\n",
    "\n",
    "# Create (1-1/7) train and 1/7 val for Split #3\n",
    "# We want 70% train and 10% val\n",
    "sexual_orientation_split3_train_df, sexual_orientation_split3_val_df = train_test_split(sexual_orientation_combined_split3_df,\n",
    "                                       test_size=1/7,\n",
    "                                       random_state=266, stratify=sexual_orientation_combined_split3_df['sexual_orientation_subtype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f860c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "How big is each sexual orientation split?\n",
      "\n",
      "len(sexual_orientation_split1_train_df):  4266\n",
      "len(sexual_orientation_split2_train_df):  4266\n",
      "len(sexual_orientation_split3_train_df):  4266\n",
      "len(sexual_orientation_split1_val_df):  711\n",
      "len(sexual_orientation_split2_val_df):  711\n",
      "len(sexual_orientation_split3_val_df):  712\n",
      "total sexual_orientation train:  12798\n",
      "total sexual_orientation val:  2134\n",
      "len(sexual_orientation_test_df):  3733\n"
     ]
    }
   ],
   "source": [
    "print('\\nHow big is each sexual orientation split?\\n')\n",
    "print('len(sexual_orientation_split1_train_df): ', len(sexual_orientation_split1_train_df))\n",
    "print('len(sexual_orientation_split2_train_df): ', len(sexual_orientation_split2_train_df))\n",
    "print('len(sexual_orientation_split3_train_df): ', len(sexual_orientation_split3_train_df))\n",
    "\n",
    "print('len(sexual_orientation_split1_val_df): ', len(sexual_orientation_split1_val_df))\n",
    "print('len(sexual_orientation_split2_val_df): ', len(sexual_orientation_split2_val_df))\n",
    "print('len(sexual_orientation_split3_val_df): ', len(sexual_orientation_split3_val_df))\n",
    "\n",
    "print('total sexual_orientation train: ', len(sexual_orientation_split1_train_df)+len(sexual_orientation_split2_train_df)+len(sexual_orientation_split3_train_df))\n",
    "print('total sexual_orientation val: ', len(sexual_orientation_split1_val_df)+len(sexual_orientation_split2_val_df)+len(sexual_orientation_split3_val_df))\n",
    "print('len(sexual_orientation_test_df): ', len(sexual_orientation_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b08c1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sexual_orientation_train_df = pd.concat([sexual_orientation_split1_train_df, sexual_orientation_split2_train_df, sexual_orientation_split3_train_df], axis=0)\n",
    "sexual_orientation_val_df = pd.concat([sexual_orientation_split1_val_df, sexual_orientation_split2_val_df, sexual_orientation_split3_val_df], axis=0)\n",
    "sexual_orientation_full_df = pd.concat([sexual_orientation_train_df, sexual_orientation_val_df, sexual_orientation_test_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2df8639a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sexual_orientation_train_df):  12798\n",
      "len(sexual_orientation_val_df):  2134\n",
      "len(sexual_orientation_test_df):  3733\n",
      "len(sexual_orientation_full_df):  18665\n"
     ]
    }
   ],
   "source": [
    "print('len(sexual_orientation_train_df): ', len(sexual_orientation_train_df))\n",
    "print('len(sexual_orientation_val_df): ', len(sexual_orientation_val_df))\n",
    "print('len(sexual_orientation_test_df): ', len(sexual_orientation_test_df))\n",
    "print('len(sexual_orientation_full_df): ', len(sexual_orientation_full_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9842549e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stratified Sampling Sanity Check for Sexual Orientation\n",
      "\n",
      "Split 1 sexual_orientation Train\n",
      "=====================\n",
      "hetero:\t 0.10150023441162681\n",
      "homo:\t 0.7100328176277544\n",
      "bi:\t 0.04195968120018753\n",
      "other_sexual_orientation:\t 0.14650726676043133\n",
      "\n",
      "Split 1 sexual_orientation Val\n",
      "=====================\n",
      "hetero:\t 0.10126582278481013\n",
      "homo:\t 0.710267229254571\n",
      "bi:\t 0.04219409282700422\n",
      "other_sexual_orientation:\t 0.14627285513361463\n"
     ]
    }
   ],
   "source": [
    "print('\\nStratified Sampling Sanity Check for Sexual Orientation')\n",
    "sexual_orientation_train_hetero = (sexual_orientation_split1_train_df['sexual_orientation_subtype']=='heterosexual').astype(int).sum()\n",
    "sexual_orientation_train_homo = (sexual_orientation_split1_train_df['sexual_orientation_subtype']=='homosexual_gay_or_lesbian').astype(int).sum()\n",
    "sexual_orientation_train_bi = (sexual_orientation_split1_train_df['sexual_orientation_subtype']=='bisexual').astype(int).sum()\n",
    "sexual_orientation_train_other = (sexual_orientation_split1_train_df['sexual_orientation_subtype']=='other_sexual_orientation').astype(int).sum()\n",
    "sexual_orientation_train_total = len(sexual_orientation_split1_train_df['sexual_orientation_subtype'])\n",
    "print('\\nSplit 1 sexual_orientation Train')\n",
    "print('=====================')\n",
    "print('hetero:\\t', sexual_orientation_train_hetero/sexual_orientation_train_total)\n",
    "print('homo:\\t', sexual_orientation_train_homo/sexual_orientation_train_total)\n",
    "print('bi:\\t', sexual_orientation_train_bi/sexual_orientation_train_total)\n",
    "print('other_sexual_orientation:\\t', sexual_orientation_train_other/sexual_orientation_train_total)\n",
    "\n",
    "sexual_orientation_val_hetero = (sexual_orientation_split1_val_df['sexual_orientation_subtype']=='heterosexual').astype(int).sum()\n",
    "sexual_orientation_val_homo = (sexual_orientation_split1_val_df['sexual_orientation_subtype']=='homosexual_gay_or_lesbian').astype(int).sum()\n",
    "sexual_orientation_val_bi = (sexual_orientation_split1_val_df['sexual_orientation_subtype']=='bisexual').astype(int).sum()\n",
    "sexual_orientation_val_other = (sexual_orientation_split1_val_df['sexual_orientation_subtype']=='other_sexual_orientation').astype(int).sum()\n",
    "sexual_orientation_val_total = len(sexual_orientation_split1_val_df['sexual_orientation_subtype'])\n",
    "print('\\nSplit 1 sexual_orientation Val')\n",
    "print('=====================')\n",
    "print('hetero:\\t', sexual_orientation_val_hetero/sexual_orientation_val_total)\n",
    "print('homo:\\t', sexual_orientation_val_homo/sexual_orientation_val_total)\n",
    "print('bi:\\t', sexual_orientation_val_bi/sexual_orientation_val_total)\n",
    "print('other_sexual_orientation:\\t', sexual_orientation_val_other/sexual_orientation_val_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7a6b6d",
   "metadata": {},
   "source": [
    "### Export sexual orientation split datasets into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bef9bdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sexual_orientation_full_df.to_csv('data/sexual_orientation-dataset-full.csv')\n",
    "sexual_orientation_train_df.to_csv('data/sexual_orientation-dataset-train.csv')\n",
    "sexual_orientation_val_df.to_csv('data/sexual_orientation-dataset-val.csv')\n",
    "sexual_orientation_test_df.to_csv('data/sexual_orientation-dataset-test.csv')\n",
    "\n",
    "sexual_orientation_split1_train_df.to_csv('data/sexual_orientation-dataset-split1-train.csv')\n",
    "sexual_orientation_split2_train_df.to_csv('data/sexual_orientation-dataset-split2-train.csv')\n",
    "sexual_orientation_split3_train_df.to_csv('data/sexual_orientation-dataset-split3-train.csv')\n",
    "\n",
    "sexual_orientation_split1_val_df.to_csv('data/sexual_orientation-dataset-split1-val.csv')\n",
    "sexual_orientation_split2_val_df.to_csv('data/sexual_orientation-dataset-split2-val.csv')\n",
    "sexual_orientation_split3_val_df.to_csv('data/sexual_orientation-dataset-split3-val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e635cc",
   "metadata": {},
   "source": [
    "## Religion subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6513c5f6",
   "metadata": {},
   "source": [
    "Create religion subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67b2cc77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137722, 28)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "religion_df = all_data_df_cleansed[(all_data_df_cleansed['christian'] > 0) | \n",
    "           (all_data_df_cleansed['jewish'] > 0) | \n",
    "           (all_data_df_cleansed['muslim'] > 0) | \n",
    "           (all_data_df_cleansed['hindu'] > 0) | \n",
    "           (all_data_df_cleansed['buddhist'] > 0) | \n",
    "           (all_data_df_cleansed['atheist'] > 0) | \n",
    "           (all_data_df_cleansed['other_religion'] > 0)]\n",
    "gender_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c724102a",
   "metadata": {},
   "source": [
    "Add religion subtype column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f694fbb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxicity_binary</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "      <th>transgender</th>\n",
       "      <th>other_gender</th>\n",
       "      <th>heterosexual</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>...</th>\n",
       "      <th>black</th>\n",
       "      <th>white</th>\n",
       "      <th>asian</th>\n",
       "      <th>latino</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>religion_subtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7678</th>\n",
       "      <td>OH yes - Were those evil Christian Missionarie...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7689</th>\n",
       "      <td>Lela, you admit no records exist to support yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>atheist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7701</th>\n",
       "      <td>The robot censor seems disinclined to accept s...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7704</th>\n",
       "      <td>Agreed: there's no equivalence. What is stoppi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7716</th>\n",
       "      <td>It took them long enough. And it goes against ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>christian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           comment_text  toxicity_binary   \n",
       "7678  OH yes - Were those evil Christian Missionarie...                1  \\\n",
       "7689  Lela, you admit no records exist to support yo...                0   \n",
       "7701  The robot censor seems disinclined to accept s...                1   \n",
       "7704  Agreed: there's no equivalence. What is stoppi...                1   \n",
       "7716  It took them long enough. And it goes against ...                0   \n",
       "\n",
       "      toxicity  male  female  transgender  other_gender  heterosexual   \n",
       "7678  0.800000   0.0     0.0          0.0           0.0           0.0  \\\n",
       "7689  0.111111   0.0     0.0          0.0           0.0           0.0   \n",
       "7701  0.800000   0.0     0.0          0.0           0.0           0.0   \n",
       "7704  0.800000   0.0     0.0          0.0           0.0           0.0   \n",
       "7716  0.181818   0.0     0.0          0.0           0.0           0.0   \n",
       "\n",
       "      homosexual_gay_or_lesbian  bisexual  ...  black  white  asian  latino   \n",
       "7678                        0.0       0.0  ...    0.0   0.00    0.0     0.0  \\\n",
       "7689                        0.0       0.0  ...    0.0   0.00    0.0     0.0   \n",
       "7701                        0.0       0.0  ...    1.0   0.75    0.0     0.0   \n",
       "7704                        0.0       0.0  ...    0.0   0.00    0.0     0.0   \n",
       "7716                        1.0       0.0  ...    0.0   0.00    0.0     0.0   \n",
       "\n",
       "      other_race_or_ethnicity  physical_disability   \n",
       "7678                      0.0                  0.0  \\\n",
       "7689                      0.0                  0.0   \n",
       "7701                      0.0                  0.0   \n",
       "7704                      0.0                  0.0   \n",
       "7716                      0.0                  0.0   \n",
       "\n",
       "      intellectual_or_learning_disability  psychiatric_or_mental_illness   \n",
       "7678                                  0.0                            0.0  \\\n",
       "7689                                  0.0                            0.0   \n",
       "7701                                  0.0                            0.0   \n",
       "7704                                  0.0                            0.0   \n",
       "7716                                  0.0                            0.0   \n",
       "\n",
       "      other_disability  religion_subtype  \n",
       "7678               0.0         christian  \n",
       "7689               0.0           atheist  \n",
       "7701               0.0            muslim  \n",
       "7704               0.0         christian  \n",
       "7716               0.0         christian  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "religion_subtypes_df = religion_df[['christian','jewish','muslim','hindu','buddhist','atheist','other_religion']]\n",
    "religion_df = religion_df.assign(religion_subtype=religion_subtypes_df.idxmax(axis=1))\n",
    "religion_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0443be6b",
   "metadata": {},
   "source": [
    "### Prepare data splits for religion\n",
    "**Overview:** We want the religion subset to have the following ratio: 70% train, 10% val, and 20% test.\n",
    "\n",
    "**Disability+religion Interweaving Technique:** Each split should have stratified sampling. We'll divide the disabilty subset into 3 sets stratified on disability subtype, and divide the religion disability subset into 3 sets stratified on religion subtype. Then we'll alternate between each disability split and religion split when fine-tuning (i.e. train on disability split 1, then train on religion split 1, then train on disability split 2, then train on religion split 2, etc.). For the alternating technique, we'll need 3 training and validation sets for religion, and all sets will need to be stratified on the religion subtype.\n",
    "\n",
    "**Splitting Method:** To prepare the splits as previously described, we'll use the train_test_split() method and since it only creates two splits, we'll take the following steps to create all of the dataset splits:\n",
    "\n",
    "1. Split into group1: 80% for train and validation, and group2: 20% for test.\n",
    "1. No need to further split the test set, so leave it alone.\n",
    "1. Take the set from step 1 that combines train and validation, and split it into group1: 2/3 and group2: 1/3.\n",
    "1. Take the set from step 3 that was 2/3 and split it into half.\n",
    "1. Now we have 3 equal splits from step 3 and step 4.\n",
    "1. For each of the 3 splits, create a train and validation split. Since we want the overall ratio to be 70% train and 10% validation, the ratio for train here should be (1-1/7) and for validation it should be 1/7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09f50333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into 80% combined for train and val, and 20% test\n",
    "religion_combined_df, religion_test_df = train_test_split(religion_df,\n",
    "                                       train_size=num_disability_train_samples+num_disability_val_samples,\n",
    "                                       test_size=num_disability_test_samples,\n",
    "                                       random_state=266, stratify=religion_df['religion_subtype'])\n",
    "\n",
    "# Split the 80% train and val into: 2/3 and 1/3\n",
    "religion_combined_split12_df, religion_combined_split3_df = train_test_split(religion_combined_df,\n",
    "                                       test_size=1/3,\n",
    "                                       random_state=266, stratify=religion_combined_df['religion_subtype'])\n",
    "\n",
    "# Split the 2/3 train and val into half\n",
    "religion_combined_split1_df, religion_combined_split2_df = train_test_split(religion_combined_split12_df,\n",
    "                                       test_size=0.5,\n",
    "                                       random_state=266, stratify=religion_combined_split12_df['religion_subtype'])\n",
    "\n",
    "# Create (1-1/7) train and 1/7 val for Split #1\n",
    "# We want 70% train and 10% val\n",
    "religion_split1_train_df, religion_split1_val_df = train_test_split(religion_combined_split1_df,\n",
    "                                       test_size=1/7,\n",
    "                                       random_state=266, stratify=religion_combined_split1_df['religion_subtype'])\n",
    "\n",
    "# Create (1-1/7) train and 1/7 val for Split #2\n",
    "# We want 70% train and 10% val\n",
    "religion_split2_train_df, religion_split2_val_df = train_test_split(religion_combined_split2_df,\n",
    "                                       test_size=1/7,\n",
    "                                       random_state=266, stratify=religion_combined_split2_df['religion_subtype'])\n",
    "\n",
    "# Create (1-1/7) train and 1/7 val for Split #3\n",
    "# We want 70% train and 10% val\n",
    "religion_split3_train_df, religion_split3_val_df = train_test_split(religion_combined_split3_df,\n",
    "                                       test_size=1/7,\n",
    "                                       random_state=266, stratify=religion_combined_split3_df['religion_subtype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9bb160e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "How big is each split for religion?\n",
      "\n",
      "len(religion_split1_train_df):  4266\n",
      "len(religion_split2_train_df):  4266\n",
      "len(religion_split3_train_df):  4266\n",
      "len(religion_split1_val_df):  711\n",
      "len(religion_split2_val_df):  711\n",
      "len(religion_split3_val_df):  712\n",
      "total religion train:  12798\n",
      "total religion val:  2134\n",
      "len(religion_test_df):  3733\n"
     ]
    }
   ],
   "source": [
    "print('\\nHow big is each split for religion?\\n')\n",
    "print('len(religion_split1_train_df): ', len(religion_split1_train_df))\n",
    "print('len(religion_split2_train_df): ', len(religion_split2_train_df))\n",
    "print('len(religion_split3_train_df): ', len(religion_split3_train_df))\n",
    "\n",
    "print('len(religion_split1_val_df): ', len(religion_split1_val_df))\n",
    "print('len(religion_split2_val_df): ', len(religion_split2_val_df))\n",
    "print('len(religion_split3_val_df): ', len(religion_split3_val_df))\n",
    "\n",
    "print('total religion train: ', len(religion_split1_train_df)+len(religion_split2_train_df)+len(religion_split3_train_df))\n",
    "print('total religion val: ', len(religion_split1_val_df)+len(religion_split2_val_df)+len(religion_split3_val_df))\n",
    "print('len(religion_test_df): ', len(religion_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f0aed621",
   "metadata": {},
   "outputs": [],
   "source": [
    "religion_train_df = pd.concat([religion_split1_train_df, religion_split2_train_df, religion_split3_train_df], axis=0)\n",
    "religion_val_df = pd.concat([religion_split1_val_df, religion_split2_val_df, religion_split3_val_df], axis=0)\n",
    "religion_full_df = pd.concat([religion_train_df, religion_val_df, religion_test_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2376f8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(religion_train_df):  12798\n",
      "len(religion_val_df):  2134\n",
      "len(religion_test_df):  3733\n",
      "len(religion_full_df):  18665\n"
     ]
    }
   ],
   "source": [
    "print('len(religion_train_df): ', len(religion_train_df))\n",
    "print('len(religion_val_df): ', len(religion_val_df))\n",
    "print('len(religion_test_df): ', len(religion_test_df))\n",
    "print('len(religion_full_df): ', len(religion_full_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d8f6d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stratified Sampling Sanity Check for Religion\n",
      "\n",
      "Split 1 Religion Train\n",
      "=====================\n",
      "christian:\t 0.629395218002813\n",
      "jewish:\t 0.07172995780590717\n",
      "muslim:\t 0.23253633380215658\n",
      "hindu:\t 0.007735583684950774\n",
      "other_religion:\t 0.03727144866385373\n",
      "\n",
      "Split 1 Religion Val\n",
      "=====================\n",
      "christian:\t 0.630098452883263\n",
      "jewish:\t 0.07172995780590717\n",
      "muslim:\t 0.23347398030942335\n",
      "hindu:\t 0.007032348804500703\n",
      "other_religion:\t 0.03656821378340366\n"
     ]
    }
   ],
   "source": [
    "print('\\nStratified Sampling Sanity Check for Religion')\n",
    "religion_train_christian = (religion_split1_train_df['religion_subtype']=='christian').astype(int).sum()\n",
    "religion_train_jewish = (religion_split1_train_df['religion_subtype']=='jewish').astype(int).sum()\n",
    "religion_train_muslim = (religion_split1_train_df['religion_subtype']=='muslim').astype(int).sum()\n",
    "religion_train_hindu = (religion_split1_train_df['religion_subtype']=='hindu').astype(int).sum()\n",
    "religion_train_other = (religion_split1_train_df['religion_subtype']=='other_religion').astype(int).sum()\n",
    "religion_train_total = len(religion_split1_train_df['religion_subtype'])\n",
    "print('\\nSplit 1 Religion Train')\n",
    "print('=====================')\n",
    "print('christian:\\t', religion_train_christian/religion_train_total)\n",
    "print('jewish:\\t', religion_train_jewish/religion_train_total)\n",
    "print('muslim:\\t', religion_train_muslim/religion_train_total)\n",
    "print('hindu:\\t', religion_train_hindu/religion_train_total)\n",
    "print('other_religion:\\t', religion_train_other/religion_train_total)\n",
    "\n",
    "religion_val_christian = (religion_split1_val_df['religion_subtype']=='christian').astype(int).sum()\n",
    "religion_val_jewish = (religion_split1_val_df['religion_subtype']=='jewish').astype(int).sum()\n",
    "religion_val_muslim = (religion_split1_val_df['religion_subtype']=='muslim').astype(int).sum()\n",
    "religion_val_hindu = (religion_split1_val_df['religion_subtype']=='hindu').astype(int).sum()\n",
    "religion_val_other = (religion_split1_val_df['religion_subtype']=='other_religion').astype(int).sum()\n",
    "religion_val_total = len(religion_split1_val_df['religion_subtype'])\n",
    "print('\\nSplit 1 Religion Val')\n",
    "print('=====================')\n",
    "print('christian:\\t', religion_val_christian/religion_val_total)\n",
    "print('jewish:\\t', religion_val_jewish/religion_val_total)\n",
    "print('muslim:\\t', religion_val_muslim/religion_val_total)\n",
    "print('hindu:\\t', religion_val_hindu/religion_val_total)\n",
    "print('other_religion:\\t', religion_val_other/religion_val_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f94908",
   "metadata": {},
   "source": [
    "### Export religion split datasets into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "44e58d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "religion_full_df.to_csv('data/religion-dataset-full.csv')\n",
    "religion_train_df.to_csv('data/religion-dataset-train.csv')\n",
    "religion_val_df.to_csv('data/religion-dataset-val.csv')\n",
    "religion_test_df.to_csv('data/religion-dataset-test.csv')\n",
    "\n",
    "religion_split1_train_df.to_csv('data/religion-dataset-split1-train.csv')\n",
    "religion_split2_train_df.to_csv('data/religion-dataset-split2-train.csv')\n",
    "religion_split3_train_df.to_csv('data/religion-dataset-split3-train.csv')\n",
    "\n",
    "religion_split1_val_df.to_csv('data/religion-dataset-split1-val.csv')\n",
    "religion_split2_val_df.to_csv('data/religion-dataset-split2-val.csv')\n",
    "religion_split3_val_df.to_csv('data/religion-dataset-split3-val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d2dfc6",
   "metadata": {},
   "source": [
    "## Race subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7e917b",
   "metadata": {},
   "source": [
    "Create race subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c4d46ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71648, 27)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_df = all_data_df_cleansed[(all_data_df_cleansed['black'] > 0) | \n",
    "           (all_data_df_cleansed['white'] > 0) | \n",
    "           (all_data_df_cleansed['asian'] > 0) | \n",
    "           (all_data_df_cleansed['latino'] > 0) | \n",
    "           (all_data_df_cleansed['other_race_or_ethnicity'] > 0)]\n",
    "race_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d264d7",
   "metadata": {},
   "source": [
    "Add race subtype column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "10452b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxicity_binary</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "      <th>transgender</th>\n",
       "      <th>other_gender</th>\n",
       "      <th>heterosexual</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>...</th>\n",
       "      <th>black</th>\n",
       "      <th>white</th>\n",
       "      <th>asian</th>\n",
       "      <th>latino</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>race_subtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7679</th>\n",
       "      <td>Why is this black racist crap still on the G&amp;M...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.757143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7680</th>\n",
       "      <td>even up here.......BLACKS!</td>\n",
       "      <td>1</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7684</th>\n",
       "      <td>\"Let's get the black folks and the white folks...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7691</th>\n",
       "      <td>Are you a Pilgrim?\\nWhy arn't you growing your...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7696</th>\n",
       "      <td>And there it is. Our president is a white supr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.507042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           comment_text  toxicity_binary   \n",
       "7679  Why is this black racist crap still on the G&M...                1  \\\n",
       "7680                         even up here.......BLACKS!                1   \n",
       "7684  \"Let's get the black folks and the white folks...                1   \n",
       "7691  Are you a Pilgrim?\\nWhy arn't you growing your...                1   \n",
       "7696  And there it is. Our president is a white supr...                1   \n",
       "\n",
       "      toxicity  male  female  transgender  other_gender  heterosexual   \n",
       "7679  0.757143   0.0     0.0          0.0           0.0           0.0  \\\n",
       "7680  0.688525   0.0     0.0          0.0           0.0           0.0   \n",
       "7684  0.736842   0.0     0.0          0.0           0.0           0.0   \n",
       "7691  0.800000   1.0     0.0          0.0           0.0           0.0   \n",
       "7696  0.507042   0.0     0.0          0.0           0.0           0.0   \n",
       "\n",
       "      homosexual_gay_or_lesbian  bisexual  ...  black  white  asian  latino   \n",
       "7679                        0.0       0.0  ...    1.0   0.75    0.0     0.0  \\\n",
       "7680                        0.0       0.0  ...    1.0   0.00    0.0     0.0   \n",
       "7684                        0.0       0.0  ...    1.0   1.00    0.0     0.0   \n",
       "7691                        0.0       0.0  ...    0.0   1.00    0.0     0.0   \n",
       "7696                        0.0       0.0  ...    0.0   1.00    0.0     0.0   \n",
       "\n",
       "      other_race_or_ethnicity  physical_disability   \n",
       "7679                      0.0                  0.0  \\\n",
       "7680                      0.0                  0.0   \n",
       "7684                      0.0                  0.0   \n",
       "7691                      0.0                  0.0   \n",
       "7696                      0.0                  0.0   \n",
       "\n",
       "      intellectual_or_learning_disability  psychiatric_or_mental_illness   \n",
       "7679                                  0.0                            0.0  \\\n",
       "7680                                  0.0                            0.0   \n",
       "7684                                  0.0                            0.0   \n",
       "7691                                  0.0                            0.0   \n",
       "7696                                  0.0                            0.0   \n",
       "\n",
       "      other_disability  race_subtype  \n",
       "7679               0.0         black  \n",
       "7680               0.0         black  \n",
       "7684               0.0         black  \n",
       "7691               0.0         white  \n",
       "7696               0.0         white  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_subtypes_df = race_df[['black', 'white', 'asian','latino', 'other_race_or_ethnicity']]\n",
    "race_df = race_df.assign(race_subtype=race_subtypes_df.idxmax(axis=1))\n",
    "race_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6d853b",
   "metadata": {},
   "source": [
    "### Prepare data splits for race\n",
    "**Overview:** We want the race subset to have the following ratio: 70% train, 10% val, and 20% test.\n",
    "\n",
    "**Disability+race Interweaving Technique:** Each split should have stratified sampling. We'll divide the disabilty subset into 3 sets stratified on disability subtype, and divide the race disability subset into 3 sets stratified on race subtype. Then we'll alternate between each disability split and race split when fine-tuning (i.e. train on disability split 1, then train on race split 1, then train on disability split 2, then train on race split 2, etc.). For the alternating technique, we'll need 3 training and validation sets for race, and all sets will need to be stratified on the race subtype.\n",
    "\n",
    "**Splitting Method:** To prepare the splits as previously described, we'll use the train_test_split() method and since it only creates two splits, we'll take the following steps to create all of the dataset splits:\n",
    "\n",
    "1. Split into group1: 80% for train and validation, and group2: 20% for test.\n",
    "1. No need to further split the test set, so leave it alone.\n",
    "1. Take the set from step 1 that combines train and validation, and split it into group1: 2/3 and group2: 1/3.\n",
    "1. Take the set from step 3 that was 2/3 and split it into half.\n",
    "1. Now we have 3 equal splits from step 3 and step 4.\n",
    "1. For each of the 3 splits, create a train and validation split. Since we want the overall ratio to be 70% train and 10% validation, the ratio for train here should be (1-1/7) and for validation it should be 1/7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3558a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into 80% combined for train and val, and 20% test\n",
    "race_combined_df, race_test_df = train_test_split(race_df,\n",
    "                                       train_size=num_disability_train_samples+num_disability_val_samples,\n",
    "                                       test_size=num_disability_test_samples,\n",
    "                                       random_state=266, stratify=race_df['race_subtype'])\n",
    "\n",
    "# Split the 80% train and val into: 2/3 and 1/3\n",
    "race_combined_split12_df, race_combined_split3_df = train_test_split(race_combined_df,\n",
    "                                       test_size=1/3,\n",
    "                                       random_state=266, stratify=race_combined_df['race_subtype'])\n",
    "\n",
    "# Split the 2/3 train and val into half\n",
    "race_combined_split1_df, race_combined_split2_df = train_test_split(race_combined_split12_df,\n",
    "                                       test_size=0.5,\n",
    "                                       random_state=266, stratify=race_combined_split12_df['race_subtype'])\n",
    "\n",
    "# Create (1-1/7) train and 1/7 val for Split #1\n",
    "# We want 70% train and 10% val\n",
    "race_split1_train_df, race_split1_val_df = train_test_split(race_combined_split1_df,\n",
    "                                       test_size=1/7,\n",
    "                                       random_state=266, stratify=race_combined_split1_df['race_subtype'])\n",
    "\n",
    "# Create (1-1/7) train and 1/7 val for Split #2\n",
    "# We want 70% train and 10% val\n",
    "race_split2_train_df, race_split2_val_df = train_test_split(race_combined_split2_df,\n",
    "                                       test_size=1/7,\n",
    "                                       random_state=266, stratify=race_combined_split2_df['race_subtype'])\n",
    "\n",
    "# Create (1-1/7) train and 1/7 val for Split #3\n",
    "# We want 70% train and 10% val\n",
    "race_split3_train_df, race_split3_val_df = train_test_split(race_combined_split3_df,\n",
    "                                       test_size=1/7,\n",
    "                                       random_state=266, stratify=race_combined_split3_df['race_subtype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e62a9430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "How big is each split for race?\n",
      "\n",
      "len(race_split1_train_df):  4266\n",
      "len(race_split2_train_df):  4266\n",
      "len(race_split3_train_df):  4266\n",
      "len(race_split1_val_df):  711\n",
      "len(race_split2_val_df):  711\n",
      "len(race_split3_val_df):  712\n",
      "total race train:  12798\n",
      "total race val:  2134\n",
      "len(race_test_df):  3733\n"
     ]
    }
   ],
   "source": [
    "print('\\nHow big is each split for race?\\n')\n",
    "print('len(race_split1_train_df): ', len(race_split1_train_df))\n",
    "print('len(race_split2_train_df): ', len(race_split2_train_df))\n",
    "print('len(race_split3_train_df): ', len(race_split3_train_df))\n",
    "\n",
    "print('len(race_split1_val_df): ', len(race_split1_val_df))\n",
    "print('len(race_split2_val_df): ', len(race_split2_val_df))\n",
    "print('len(race_split3_val_df): ', len(race_split3_val_df))\n",
    "\n",
    "print('total race train: ', len(race_split1_train_df)+len(race_split2_train_df)+len(race_split3_train_df))\n",
    "print('total race val: ', len(race_split1_val_df)+len(race_split2_val_df)+len(race_split3_val_df))\n",
    "print('len(race_test_df): ', len(race_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26b271db",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_train_df = pd.concat([race_split1_train_df, race_split2_train_df, race_split3_train_df], axis=0)\n",
    "race_val_df = pd.concat([race_split1_val_df, race_split2_val_df, race_split3_val_df], axis=0)\n",
    "race_full_df = pd.concat([race_train_df, race_val_df, race_test_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f6ceabb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(race_train_df):  12798\n",
      "len(race_val_df):  2134\n",
      "len(race_test_df):  3733\n",
      "len(race_full_df):  18665\n"
     ]
    }
   ],
   "source": [
    "print('len(race_train_df): ', len(race_train_df))\n",
    "print('len(race_val_df): ', len(race_val_df))\n",
    "print('len(race_test_df): ', len(race_test_df))\n",
    "print('len(race_full_df): ', len(race_full_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8555d415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stratified Sampling Sanity Check for race\n",
      "\n",
      "Split 1 race Train\n",
      "=====================\n",
      "black:\t 0.24917955930614158\n",
      "white:\t 0.3663853727144866\n",
      "asian:\t 0.13877168307548055\n",
      "latino:\t 0.07196436943272386\n",
      "other_race_or_ethnicity:\t 0.17369901547116737\n",
      "\n",
      "Split 1 race Val\n",
      "=====================\n",
      "black:\t 0.2489451476793249\n",
      "white:\t 0.3656821378340366\n",
      "asian:\t 0.13924050632911392\n",
      "latino:\t 0.07172995780590717\n",
      "other_race_or_ethnicity:\t 0.17440225035161744\n"
     ]
    }
   ],
   "source": [
    "print('\\nStratified Sampling Sanity Check for race')\n",
    "race_train_black = (race_split1_train_df['race_subtype']=='black').astype(int).sum()\n",
    "race_train_white = (race_split1_train_df['race_subtype']=='white').astype(int).sum()\n",
    "race_train_asian = (race_split1_train_df['race_subtype']=='asian').astype(int).sum()\n",
    "race_train_latino = (race_split1_train_df['race_subtype']=='latino').astype(int).sum()\n",
    "race_train_other = (race_split1_train_df['race_subtype']=='other_race_or_ethnicity').astype(int).sum()\n",
    "race_train_total = len(race_split1_train_df['race_subtype'])\n",
    "print('\\nSplit 1 race Train')\n",
    "print('=====================')\n",
    "print('black:\\t', race_train_black/race_train_total)\n",
    "print('white:\\t', race_train_white/race_train_total)\n",
    "print('asian:\\t', race_train_asian/race_train_total)\n",
    "print('latino:\\t', race_train_latino/race_train_total)\n",
    "print('other_race_or_ethnicity:\\t', race_train_other/race_train_total)\n",
    "\n",
    "race_val_black = (race_split1_val_df['race_subtype']=='black').astype(int).sum()\n",
    "race_val_white = (race_split1_val_df['race_subtype']=='white').astype(int).sum()\n",
    "race_val_asian = (race_split1_val_df['race_subtype']=='asian').astype(int).sum()\n",
    "race_val_latino = (race_split1_val_df['race_subtype']=='latino').astype(int).sum()\n",
    "race_val_other = (race_split1_val_df['race_subtype']=='other_race_or_ethnicity').astype(int).sum()\n",
    "race_val_total = len(race_split1_val_df['race_subtype'])\n",
    "print('\\nSplit 1 race Val')\n",
    "print('=====================')\n",
    "print('black:\\t', race_val_black/race_val_total)\n",
    "print('white:\\t', race_val_white/race_val_total)\n",
    "print('asian:\\t', race_val_asian/race_val_total)\n",
    "print('latino:\\t', race_val_latino/race_val_total)\n",
    "print('other_race_or_ethnicity:\\t', race_val_other/race_val_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65971a77",
   "metadata": {},
   "source": [
    "### Export race split datasets into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fee82a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_full_df.to_csv('data/race-dataset-full.csv')\n",
    "race_train_df.to_csv('data/race-dataset-train.csv')\n",
    "race_val_df.to_csv('data/race-dataset-val.csv')\n",
    "race_test_df.to_csv('data/race-dataset-test.csv')\n",
    "\n",
    "race_split1_train_df.to_csv('data/race-dataset-split1-train.csv')\n",
    "race_split2_train_df.to_csv('data/race-dataset-split2-train.csv')\n",
    "race_split3_train_df.to_csv('data/race-dataset-split3-train.csv')\n",
    "\n",
    "race_split1_val_df.to_csv('data/race-dataset-split1-val.csv')\n",
    "race_split2_val_df.to_csv('data/race-dataset-split2-val.csv')\n",
    "race_split3_val_df.to_csv('data/race-dataset-split3-val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf97be94",
   "metadata": {},
   "source": [
    "# Reference code: How to load split data for use in our models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cfa5c1",
   "metadata": {},
   "source": [
    "Load the csv files for each data split with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45239be",
   "metadata": {},
   "outputs": [],
   "source": [
    "disability_df_train = pd.read_csv('data/disability-dataset-train.csv')\n",
    "disability_df_val = pd.read_csv('data/disability-dataset-val.csv')\n",
    "disability_df_test = pd.read_csv('data/disability-dataset-test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effdefcb",
   "metadata": {},
   "source": [
    "Now that we loaded our data, we'll need their labels and text examples in the form of tensors. Use the code below to accomplish this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99c6ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form tensors of labels and features.\n",
    "disability_train_labels = tf.convert_to_tensor(disability_df_train['toxicity_binary'])\n",
    "disability_val_labels = tf.convert_to_tensor(disability_df_val['toxicity_binary'])\n",
    "disability_test_labels = tf.convert_to_tensor(disability_df_test['toxicity_binary'])\n",
    "\n",
    "disability_train_examples = tf.convert_to_tensor(disability_df_train['comment_text'])\n",
    "disability_val_examples = tf.convert_to_tensor(disability_df_val['comment_text'])\n",
    "disability_test_examples = tf.convert_to_tensor(disability_df_test['comment_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "w266tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
