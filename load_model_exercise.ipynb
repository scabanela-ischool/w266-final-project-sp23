{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5bb734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import transformers\n",
    "from transformers import AutoTokenizer,TFRobertaModel\n",
    "\n",
    "import pytz\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2d32521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15a41fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: macOS-12.5-arm64-arm-64bit\n",
      "Tensor Flow Version: 2.12.0\n",
      "Keras Version: 2.12.0\n",
      "\n",
      "Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:12:31) [Clang 14.0.6 ]\n",
      "Pandas 2.0.0\n",
      "Scikit-Learn 1.2.2\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# What version of Python do you have?\n",
    "import sys\n",
    "import platform\n",
    "import sklearn as sk\n",
    "\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tf.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c662aa00",
   "metadata": {},
   "source": [
    "## Functions and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3087faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_arrays(df):\n",
    "  X = df['comment_text'].to_numpy()\n",
    "  y = df['toxicity_binary'].to_numpy()\n",
    "  return X, y\n",
    "\n",
    "def load_data(group):\n",
    "  df_train = pd.read_csv('data/' + group + '-dataset-train.csv')\n",
    "  df_val = pd.read_csv('data/' + group + '-dataset-val.csv')\n",
    "  df_test = pd.read_csv('data/' + group + '-dataset-test.csv')\n",
    "\n",
    "  X_train, y_train = to_arrays(df_train)\n",
    "  X_val, y_val = to_arrays(df_val)\n",
    "  X_test, y_test = to_arrays(df_test)\n",
    "\n",
    "  return X_train, y_train, X_test, y_test, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03dd75b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5da95e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing_pipeline(X, tokenizer):\n",
    "  bert_tokenized = tokenizer(list(X),\n",
    "                max_length=MAX_SEQUENCE_LENGTH,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_tensors='tf')\n",
    "  bert_inputs = [bert_tokenized.input_ids,\n",
    "                 bert_tokenized.token_type_ids,\n",
    "                 bert_tokenized.attention_mask]\n",
    "  return bert_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89fe754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bertweet_cls_model(max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "                          hidden_size=100, \n",
    "                          dropout=0.3,\n",
    "                          learning_rate=0.0001,\n",
    "                          num_train_layers=0):\n",
    "\n",
    "    # freeze all pre-trained BERTweet layers\n",
    "    if num_train_layers == 0:\n",
    "      bertweet_model.trainable = False\n",
    "\n",
    "    # partially freeze the first n pre-trained BERTweet layers\n",
    "    else:\n",
    "        for layer_num in range(num_train_layers):\n",
    "            bertweet_model.roberta.encoder.layer[layer_num].trainable = False\n",
    "    \n",
    "    input_ids = tf.keras.layers.Input(shape=(max_sequence_length,), dtype=tf.int64, name='input_ids_layer')\n",
    "    token_type_ids = tf.keras.layers.Input(shape=(max_sequence_length,), dtype=tf.int64, name='token_type_ids_layer')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_sequence_length,), dtype=tf.int64, name='attention_mask_layer')\n",
    "\n",
    "    bert_inputs = {'input_ids': input_ids,\n",
    "                   'token_type_ids': token_type_ids,\n",
    "                   'attention_mask': attention_mask}      \n",
    "\n",
    "    # Use the same bertweet model instance\n",
    "    bert_out = bertweet_model(bert_inputs)\n",
    "\n",
    "    cls_token = bert_out[0][:, 0, :]\n",
    "\n",
    "    \n",
    "    hidden = tf.keras.layers.Dense(hidden_size, activation='relu', name='hidden_layer')(cls_token)\n",
    "\n",
    "    hidden = tf.keras.layers.Dropout(dropout)(hidden)  \n",
    "\n",
    "    f1_score = tfa.metrics.F1Score(1, threshold = 0.5)\n",
    "\n",
    "    classification = tf.keras.layers.Dense(1, activation='sigmoid', name='classification_layer')(hidden)\n",
    "    \n",
    "    classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=[classification])\n",
    "    \n",
    "    classification_model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate),\n",
    "                                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), \n",
    "                                 metrics=[tf.keras.metrics.BinaryAccuracy(),\n",
    "                                        tf.keras.metrics.Precision(),\n",
    "                                        tf.keras.metrics.Recall(),\n",
    "                                        f1_score])\n",
    "\n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c643d2",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bedf5bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_disability, y_train_disability, X_test_disability, y_test_disability, X_val_disability, y_val_disability = load_data('disability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d1056cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Doesn't work? How do we know? When the country is at the point of legalizing silencers, and the right of the mentally ill to own assault weapons, it's laughable to think we have ANY gun controls.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_disability[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "333f691b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LoL. The mental retardation of the (d)onkeys is stunning.\\nThey propose the craziest whackjob laws without one regard to the Constitution.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_disability[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7d2aef",
   "metadata": {},
   "source": [
    "## Load BERTweet Model from_pretrained() with normalization=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c37477e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some layers from the model checkpoint at vinai/bertweet-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at vinai/bertweet-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# For transformers v4.x+:\n",
    "# bertweet_tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
    "# bertweet_tokenizer_heavy = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\",\n",
    "#                                                     use_fast=False,\n",
    "#                                                     normalization=True,\n",
    "#                                                     add_special_tokens=True,\n",
    "#                                                     return_attention_mask=True)\n",
    "bertweet_tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\",\n",
    "                                                    use_fast=False,\n",
    "                                                    normalization=True,\n",
    "                                                    add_special_tokens=True,\n",
    "                                                    return_attention_mask=True)\n",
    "bertweet_model = TFRobertaModel.from_pretrained(\"vinai/bertweet-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41a6d42",
   "metadata": {},
   "source": [
    "## Tokenize Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3a8d66ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertweet_train_inputs_disability = tokenizing_pipeline(X_train_disability, bertweet_tokenizer)\n",
    "bertweet_test_inputs_disability = tokenizing_pipeline(X_test_disability, bertweet_tokenizer)\n",
    "bertweet_val_inputs_disability = tokenizing_pipeline(X_val_disability, bertweet_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205bfdb6",
   "metadata": {},
   "source": [
    "## Calculate Class Weights for Disability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b26baee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disability Examples:\n",
      "    Total: 12798\n",
      "    Positive: 2758 (21.55% of total)\n",
      "\n",
      "Disability Weight for class 0: 0.64\n",
      "Disability Weight for class 1: 2.32\n"
     ]
    }
   ],
   "source": [
    "neg, pos = np.bincount(y_train_disability)\n",
    "total = neg + pos\n",
    "print('Disability Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "    total, pos, 100 * pos / total))\n",
    "\n",
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "weight_for_0 = (1 / neg) * (total / 2.0)\n",
    "weight_for_1 = (1 / pos) * (total / 2.0)\n",
    "\n",
    "disability_class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Disability Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Disability Weight for class 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7576a5bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/cabanela/anaconda3/envs/w266tensorflow/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 942, in run_call_with_unpacked_inputs  *\n        unpacked_inputs = input_processing(func, config, **fn_args_and_kwargs)\n    File \"/Users/cabanela/anaconda3/envs/w266tensorflow/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 498, in input_processing  *\n        raise ValueError(\n\n    ValueError: The following keyword arguments are not supported by this model: ['name'].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m disability_model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_bertweet_cls_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_train_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m disability_model\u001b[38;5;241m.\u001b[39msummary()\n",
      "Cell \u001b[0;32mIn[7], line 25\u001b[0m, in \u001b[0;36mbuild_bertweet_cls_model\u001b[0;34m(max_sequence_length, hidden_size, dropout, learning_rate, num_train_layers)\u001b[0m\n\u001b[1;32m     20\u001b[0m bert_inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: input_ids,\n\u001b[1;32m     21\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: token_type_ids,\n\u001b[1;32m     22\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: attention_mask}      \n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Use the same bertweet model instance\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m bert_out \u001b[38;5;241m=\u001b[39m \u001b[43mbertweet_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbert_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbertweet_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m cls_token \u001b[38;5;241m=\u001b[39m bert_out[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m0\u001b[39m, :]\n\u001b[1;32m     30\u001b[0m hidden \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(hidden_size, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_layer\u001b[39m\u001b[38;5;124m'\u001b[39m)(cls_token)\n",
      "File \u001b[0;32m~/anaconda3/envs/w266tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/kq/0vnkqgnd0yn7k1p7838hnbd00000gn/T/__autograph_generated_filea7hl6bl6.py:33\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m config \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     32\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEncoderDecoder\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, if_body, else_body, get_state, set_state, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_processing\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn_args_and_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/kq/0vnkqgnd0yn7k1p7838hnbd00000gn/T/__autograph_generated_fileiryywwdr.py:113\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__input_processing\u001b[0;34m(func, config, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m     ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mlen\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(kwargs)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkwargs_call\u001b[39m\u001b[38;5;124m'\u001b[39m],), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, if_body_4, else_body_4, get_state_4, set_state_4, (), \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    112\u001b[0m     ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(kwargs)\u001b[38;5;241m.\u001b[39mpop, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkwargs_call\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m--> 113\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhas_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_body_5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melse_body_5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state_5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state_5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput[\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkwargs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_7\u001b[39m():\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ()\n",
      "File \u001b[0;32m/var/folders/kq/0vnkqgnd0yn7k1p7838hnbd00000gn/T/__autograph_generated_fileiryywwdr.py:111\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__input_processing.<locals>.else_body_5\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21melse_body_4\u001b[39m():\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkwargs_call\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_body_4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melse_body_4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state_4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state_4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(kwargs)\u001b[38;5;241m.\u001b[39mpop, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkwargs_call\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/var/folders/kq/0vnkqgnd0yn7k1p7838hnbd00000gn/T/__autograph_generated_fileiryywwdr.py:107\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__input_processing.<locals>.else_body_5.<locals>.if_body_4\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mif_body_4\u001b[39m():\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;167;01mValueError\u001b[39;00m), (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following keyword arguments are not supported by this model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mlist\u001b[39m),\u001b[38;5;250m \u001b[39m(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(kwargs)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkwargs_call\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys,\u001b[38;5;250m \u001b[39m(),\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;250m \u001b[39mfscope),),\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;250m \u001b[39mfscope)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/cabanela/anaconda3/envs/w266tensorflow/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 942, in run_call_with_unpacked_inputs  *\n        unpacked_inputs = input_processing(func, config, **fn_args_and_kwargs)\n    File \"/Users/cabanela/anaconda3/envs/w266tensorflow/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 498, in input_processing  *\n        raise ValueError(\n\n    ValueError: The following keyword arguments are not supported by this model: ['name'].\n"
     ]
    }
   ],
   "source": [
    "disability_model = build_bertweet_cls_model(num_train_layers=6, learning_rate=1e-4)\n",
    "disability_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1631acc",
   "metadata": {},
   "source": [
    "## Train Disability Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2d1058",
   "metadata": {},
   "outputs": [],
   "source": [
    "disability_history = disability_model.fit(bertweet_train_inputs_disability,\n",
    "                                          y_train_disability,\n",
    "                                          validation_data=(bertweet_val_inputs_disability, y_val_disability),\n",
    "                                          batch_size=64,\n",
    "                                          epochs=1,\n",
    "                                          class_weight=disability_class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f43129",
   "metadata": {},
   "source": [
    "## Evaluate Disability Model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdb3b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "disability_test_history = disability_model.evaluate(bertweet_test_inputs_disability, y_test_disability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f72f54",
   "metadata": {},
   "source": [
    "# Save Model & Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "168550b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8148430",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('test_load_model_epoch1.h5')  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3916b737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identical to the previous one\n",
    "loaded_model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc6dab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to load the weights into a different architecture (with some layers in common),\n",
    "# for instance for fine-tuning or transfer-learning, you can load them by layer name:\n",
    "loadmodel.load_weights('my_model_weights.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e00131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "disability_model.load_weights('saved_weights/take2_disability_only_half_frozen.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "136f1f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " attention_mask_layer (InputLay  [(None, 128)]       0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " input_ids_layer (InputLayer)   [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " token_type_ids_layer (InputLay  [(None, 128)]       0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " tf_roberta_model_1 (TFRobertaM  TFBaseModelOutputWi  134899968  ['attention_mask_layer[0][0]',   \n",
      " odel)                          thPoolingAndCrossAt               'input_ids_layer[0][0]',        \n",
      "                                tentions(last_hidde               'token_type_ids_layer[0][0]']   \n",
      "                                n_state=(None, 128,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_5 (Sl  (None, 768)         0           ['tf_roberta_model_1[0][0]']     \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " hidden_layer (Dense)           (None, 100)          76900       ['tf.__operators__.getitem_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_79 (Dropout)           (None, 100)          0           ['hidden_layer[0][0]']           \n",
      "                                                                                                  \n",
      " classification_layer (Dense)   (None, 1)            101         ['dropout_79[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 134,976,969\n",
      "Trainable params: 92,449,737\n",
      "Non-trainable params: 42,527,232\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "random_model = build_bertweet_cls_model(num_train_layers=6, learning_rate=1e-5)\n",
    "random_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "847831ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 86s 714ms/step - loss: 0.6317 - binary_accuracy: 0.7702 - precision_5: 0.0826 - recall_5: 0.0132 - f1_score: 0.0228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6317230463027954,\n",
       " 0.770158052444458,\n",
       " 0.08264463394880295,\n",
       " 0.013210039585828781,\n",
       " array([0.02277905], dtype=float32)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-evaluate the model\n",
    "# loss, acc = random_model.evaluate(bertweet_test_inputs_disability, y_test_disability, verbose=2)\n",
    "# print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))\n",
    "random_model.evaluate(bertweet_test_inputs_disability, y_test_disability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b3debd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " attention_mask_layer (InputLay  [(None, 128)]       0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " input_ids_layer (InputLayer)   [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " token_type_ids_layer (InputLay  [(None, 128)]       0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " tf_roberta_model_1 (TFRobertaM  TFBaseModelOutputWi  134899968  ['attention_mask_layer[0][0]',   \n",
      " odel)                          thPoolingAndCrossAt               'input_ids_layer[0][0]',        \n",
      "                                tentions(last_hidde               'token_type_ids_layer[0][0]']   \n",
      "                                n_state=(None, 128,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_6 (Sl  (None, 768)         0           ['tf_roberta_model_1[1][0]']     \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " hidden_layer (Dense)           (None, 100)          76900       ['tf.__operators__.getitem_6[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_80 (Dropout)           (None, 100)          0           ['hidden_layer[0][0]']           \n",
      "                                                                                                  \n",
      " classification_layer (Dense)   (None, 1)            101         ['dropout_80[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 134,976,969\n",
      "Trainable params: 92,449,737\n",
      "Non-trainable params: 42,527,232\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model = build_bertweet_cls_model(num_train_layers=6, learning_rate=1e-5)\n",
    "loaded_model.load_weights('saved_weights/take2_disability_only_half_frozen.h5')\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f798b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/117 [==============================] - 88s 720ms/step - loss: 0.6018 - binary_accuracy: 0.7351 - precision_6: 0.4292 - recall_6: 0.9287 - f1_score: 0.5871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6017908453941345,\n",
       " 0.7350656390190125,\n",
       " 0.42918193340301514,\n",
       " 0.928665816783905,\n",
       " array([0.5870564], dtype=float32)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Re-evaluate the model\n",
    "# loss, acc = loaded_model.evaluate(bertweet_test_inputs_disability, y_test_disability, verbose=2)\n",
    "# print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))\n",
    "loaded_model.evaluate(bertweet_test_inputs_disability, y_test_disability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23509963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the weights\n",
    "model2.load_weights(checkpoint_path)\n",
    "\n",
    "# Re-evaluate the model\n",
    "loss, acc = model2.evaluate(test_images, test_labels, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))\n",
    "\n",
    "# do stuff with model2 (e.g. predict() to get y_pred)\n",
    "# Then you can do stuff like keras.metrics.accuracy(y_true, y_pred)\n",
    "# Then you can do stuff like keras.metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48db069",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1af83d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fb6a9a3",
   "metadata": {},
   "source": [
    "# HOW TO LOAD DISABILITY-ONLY WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03354417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bertweet_cls_model(max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "                          hidden_size=100, \n",
    "                          dropout=0.3,\n",
    "                          learning_rate=0.0001,\n",
    "                          num_train_layers=0):\n",
    "\n",
    "    # freeze all pre-trained BERTweet layers\n",
    "    if num_train_layers == 0:\n",
    "      bertweet_model.trainable = False\n",
    "\n",
    "    # partially freeze the first n pre-trained BERTweet layers\n",
    "    else:\n",
    "        for layer_num in range(num_train_layers):\n",
    "            bertweet_model.roberta.encoder.layer[layer_num].trainable = False\n",
    "    \n",
    "    input_ids = tf.keras.layers.Input(shape=(max_sequence_length,), dtype=tf.int64, name='input_ids_layer')\n",
    "    token_type_ids = tf.keras.layers.Input(shape=(max_sequence_length,), dtype=tf.int64, name='token_type_ids_layer')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_sequence_length,), dtype=tf.int64, name='attention_mask_layer')\n",
    "\n",
    "    bert_inputs = {'input_ids': input_ids,\n",
    "                   'token_type_ids': token_type_ids,\n",
    "                   'attention_mask': attention_mask}      \n",
    "\n",
    "    # Use the same bertweet model instance\n",
    "    bert_out = bertweet_model(bert_inputs)\n",
    "\n",
    "    cls_token = bert_out[0][:, 0, :]\n",
    "\n",
    "    \n",
    "    hidden = tf.keras.layers.Dense(hidden_size, activation='relu', name='hidden_layer')(cls_token)\n",
    "\n",
    "    hidden = tf.keras.layers.Dropout(dropout)(hidden)  \n",
    "\n",
    "    f1_score = tfa.metrics.F1Score(1, threshold = 0.5)\n",
    "\n",
    "    classification = tf.keras.layers.Dense(1, activation='sigmoid', name='classification_layer')(hidden)\n",
    "    \n",
    "    classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=[classification])\n",
    "    # before compiling, load weights from the top classification layer from the best disability-only model with same architecture\n",
    "    classification_model.load_weights(\"disability-only-weights-best.hdf5\")\n",
    "    \n",
    "    classification_model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate),\n",
    "                                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), \n",
    "                                 metrics=[tf.keras.metrics.BinaryAccuracy(),\n",
    "                                        tf.keras.metrics.Precision(),\n",
    "                                        tf.keras.metrics.Recall(),\n",
    "                                        f1_score])\n",
    "\n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e423ca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "filepath=\"weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "# Fit the model\n",
    "model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, callbacks=callbacks_list, verbose=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "w266tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
